{"cells":[{"cell_type":"markdown","metadata":{"id":"y1von-1uFPIV"},"source":["# SSD (Scratch)\n","\n","Github repo:\n","https://github.com/FurkanOM/tf-ssd/tree/master"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35016,"status":"ok","timestamp":1708752668234,"user":{"displayName":"happy duck","userId":"12590459388487510630"},"user_tz":-480},"id":"h4KJ9uuJF4sO","outputId":"4cee447c-0298-4e48-9813-851e8321180d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:01:04.343176Z","iopub.status.busy":"2024-02-14T08:01:04.342500Z","iopub.status.idle":"2024-02-14T08:01:18.049260Z","shell.execute_reply":"2024-02-14T08:01:18.048488Z","shell.execute_reply.started":"2024-02-14T08:01:04.343146Z"},"id":"5FT4SzeFBG7l","trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from PIL import Image, ImageDraw\n","import matplotlib.pyplot as plt\n","import math\n","import numpy as np\n","from datetime import datetime\n","import os\n","import pathlib\n","import argparse"]},{"cell_type":"markdown","metadata":{"id":"jCX-6w4_BG7u"},"source":["# anchor_utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:01:18.051346Z","iopub.status.busy":"2024-02-14T08:01:18.050862Z","iopub.status.idle":"2024-02-14T08:01:18.066747Z","shell.execute_reply":"2024-02-14T08:01:18.065738Z","shell.execute_reply.started":"2024-02-14T08:01:18.051319Z"},"id":"D9N9T9kpDwjD","trusted":true},"outputs":[],"source":["\"\"\"Generate Anchor Boxes.\n","\"\"\"\n","\n","def get_scale_for_nth_feature_map(k, m=6, scale_min=0.2, scale_max=0.9):\n","    \"\"\"Calculating scale value for nth feature map using the given method in the paper.\n","    inputs:\n","        k = nth feature map for scale calculation\n","        m = length of all using feature maps for detections, 6 for ssd300\n","\n","    outputs:\n","        scale = calculated scale value for given index\n","    \"\"\"\n","    # [0.2, 0.34, 0.48, 0.62, 0.76, 0.9, 1.04]\n","    return scale_min + ((scale_max - scale_min) / (m - 1)) * (k - 1)\n","\n","def generate_base_prior_boxes(aspect_ratios, feature_map_index, total_feature_map, hyper_params):\n","    \"\"\"Generating top left prior boxes for given stride, height and width pairs of different aspect ratios.\n","    These prior boxes same with the anchors in Faster-RCNN.\n","    inputs:\n","        aspect_ratios = for all feature map shapes + 1 for ratio 1\n","        feature_map_index = nth feature maps for scale calculation\n","        total_feature_map = length of all using feature map for detections, 6 for ssd300\n","\n","    outputs:\n","        base_prior_boxes = (prior_box_count, [y1, x1, y2, x2])\n","    \"\"\"\n","    # print(feature_map_index)\n","    if hyper_params[\"use_custom_scale\"]:\n","        current_scale = hyper_params[\"scale\"][feature_map_index-1]\n","        next_scale = hyper_params[\"scale\"][feature_map_index]\n","    else:\n","        current_scale = get_scale_for_nth_feature_map(feature_map_index, m=total_feature_map,\n","                                                      scale_min=hyper_params[\"scale_min\"], scale_max=hyper_params[\"scale_max\"])\n","        next_scale = get_scale_for_nth_feature_map(feature_map_index + 1, m=total_feature_map,\n","                                                   scale_min=hyper_params[\"scale_min\"], scale_max=hyper_params[\"scale_max\"])\n","    print(current_scale, next_scale)\n","    base_prior_boxes = []\n","    for aspect_ratio in aspect_ratios:\n","        height = current_scale / tf.sqrt(aspect_ratio)\n","        width = current_scale * tf.sqrt(aspect_ratio)\n","        base_prior_boxes.append([-height/2, -width/2, height/2, width/2])\n","#         print(height, width)\n","    # 1 extra pair for ratio 1\n","    height = width = tf.sqrt(current_scale * next_scale)\n","#     print(height, width)\n","    base_prior_boxes.append([-height/2, -width/2, height/2, width/2])\n","    return tf.cast(base_prior_boxes, dtype=tf.float32)\n","\n","def generate_prior_boxes(feature_map_shapes, aspect_ratios, hyper_params):\n","    \"\"\"Generating top left prior boxes for given stride, height and width pairs of different aspect ratios.\n","    These prior boxes same with the anchors in Faster-RCNN.\n","    Aspect ratio is the width to height ratio.\n","    inputs:\n","        feature_map_shapes = for all feature map output size\n","        aspect_ratios = for all feature map shapes + 1 for ratio 1\n","\n","    outputs:\n","        prior_boxes = (total_prior_boxes, [y1, x1, y2, x2])\n","            these values in normalized format between [0, 1]\n","    \"\"\"\n","    prior_boxes = []\n","    for i, feature_map_shape in enumerate(feature_map_shapes):\n","        print(feature_map_shape)\n","        base_prior_boxes = generate_base_prior_boxes(aspect_ratios[i], i+1, len(feature_map_shapes), hyper_params)\n","        print(base_prior_boxes)\n","\n","        stride = 1 / feature_map_shape\n","        # Create linearly spaced arrays of x and y coordinates\n","        grid_coords = tf.cast(tf.range(0, feature_map_shape) / feature_map_shape + stride / 2, dtype=tf.float32)\n","        grid_x, grid_y = tf.meshgrid(grid_coords, grid_coords)\n","        flat_grid_x, flat_grid_y = tf.reshape(grid_x, (-1, )), tf.reshape(grid_y, (-1, ))\n","\n","        grid_map = tf.stack([flat_grid_y, flat_grid_x, flat_grid_y, flat_grid_x], -1)\n","\n","        prior_boxes_for_feature_map = tf.reshape(base_prior_boxes, (1, -1, 4)) + tf.reshape(grid_map, (-1, 1, 4))\n","        prior_boxes_for_feature_map = tf.reshape(prior_boxes_for_feature_map, (-1, 4))\n","        prior_boxes.append(prior_boxes_for_feature_map)\n","\n","    prior_boxes = tf.concat(prior_boxes, axis=0)\n","    # print(prior_boxes)\n","    return tf.clip_by_value(prior_boxes, 0, 1)"]},{"cell_type":"markdown","metadata":{"id":"Stm5EzBc79ca"},"source":["# parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:01:18.068519Z","iopub.status.busy":"2024-02-14T08:01:18.068147Z","iopub.status.idle":"2024-02-14T08:01:18.116819Z","shell.execute_reply":"2024-02-14T08:01:18.116029Z","shell.execute_reply.started":"2024-02-14T08:01:18.068484Z"},"id":"RUJfTzYkBG76","trusted":true},"outputs":[],"source":["backbone = \"mobilenet_v2\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2024-02-14T08:01:20.281634Z","iopub.status.busy":"2024-02-14T08:01:20.280919Z","iopub.status.idle":"2024-02-14T08:01:22.571873Z","shell.execute_reply":"2024-02-14T08:01:22.570916Z","shell.execute_reply.started":"2024-02-14T08:01:20.281599Z"},"executionInfo":{"elapsed":2929,"status":"ok","timestamp":1708752676414,"user":{"displayName":"happy duck","userId":"12590459388487510630"},"user_tz":-480},"id":"AE1YUdJu7_HW","outputId":"c3cf513d-23f4-4469-cfac-61c35f5676ca","trusted":true},"outputs":[{"data":{"text/plain":["{'img_size': 640,\n"," 'feature_map_shapes': [160, 80, 40, 20, 10],\n"," 'aspect_ratios': [[1.0, 2.0, 0.5, 5.0, 0.2],\n","  [1.0, 2.0, 0.5, 5.0, 0.2],\n","  [1.0, 2.0, 0.5, 5.0, 0.2],\n","  [1.0, 2.0, 0.5, 5.0, 0.2],\n","  [1.0, 2.0, 0.5, 5.0, 0.2]],\n"," 'use_custom_scale': False,\n"," 'scale_min': 0.05,\n"," 'scale_max': 0.9,\n"," 'scale': [0.05, 0.1, 0.2, 0.4, 0.7, 1, 1.5],\n"," 'trainable': True,\n"," 'num_trainable': None,\n"," 'detection': 'BiFPN',\n"," 'feature_fusion': None,\n"," 'dataset': 0,\n"," 'iou_threshold': 0.5,\n"," 'neg_pos_ratio': 3,\n"," 'loc_loss_alpha': 1,\n"," 'variances': [0.1, 0.1, 0.2, 0.2],\n"," 'use_focal': False,\n"," 'alpha': 2.0,\n"," 'gamma': 0.25,\n"," 'batch_size': 8,\n"," 'epochs': 200,\n"," 'lr': 1e-05,\n"," 'patience': 20}"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["160\n","0.05 0.2625\n","tf.Tensor(\n","[[-0.025      -0.025       0.025       0.025     ]\n"," [-0.01767767 -0.03535534  0.01767767  0.03535534]\n"," [-0.03535534 -0.01767767  0.03535534  0.01767767]\n"," [-0.01118034 -0.0559017   0.01118034  0.0559017 ]\n"," [-0.0559017  -0.01118034  0.0559017   0.01118034]\n"," [-0.05728219 -0.05728219  0.05728219  0.05728219]], shape=(6, 4), dtype=float32)\n","80\n","0.2625 0.475\n","tf.Tensor(\n","[[-0.13125    -0.13125     0.13125     0.13125   ]\n"," [-0.09280776 -0.18561552  0.09280776  0.18561552]\n"," [-0.18561552 -0.09280776  0.18561552  0.09280776]\n"," [-0.05869679 -0.29348388  0.05869679  0.29348388]\n"," [-0.2934839  -0.05869678  0.2934839   0.05869678]\n"," [-0.17655559 -0.17655559  0.17655559  0.17655559]], shape=(6, 4), dtype=float32)\n","40\n","0.475 0.6875\n","tf.Tensor(\n","[[-0.2375     -0.2375      0.2375      0.2375    ]\n"," [-0.16793786 -0.33587572  0.16793786  0.33587572]\n"," [-0.33587572 -0.16793786  0.33587572  0.16793786]\n"," [-0.10621323 -0.53106606  0.10621323  0.53106606]\n"," [-0.5310661  -0.10621323  0.5310661   0.10621323]\n"," [-0.28572825 -0.28572825  0.28572825  0.28572825]], shape=(6, 4), dtype=float32)\n","20\n","0.6875 0.9\n","tf.Tensor(\n","[[-0.34375    -0.34375     0.34375     0.34375   ]\n"," [-0.24306796 -0.4861359   0.24306796  0.4861359 ]\n"," [-0.48613593 -0.24306795  0.48613593  0.24306795]\n"," [-0.15372969 -0.76864827  0.15372969  0.76864827]\n"," [-0.7686484  -0.15372968  0.7686484   0.15372968]\n"," [-0.3933033  -0.3933033   0.3933033   0.3933033 ]], shape=(6, 4), dtype=float32)\n","10\n","0.9 1.1125\n","tf.Tensor(\n","[[-0.45       -0.45        0.45        0.45      ]\n"," [-0.31819806 -0.63639605  0.31819806  0.63639605]\n"," [-0.6363961  -0.31819803  0.6363961   0.31819803]\n"," [-0.20124613 -1.0062305   0.20124613  1.0062305 ]\n"," [-1.0062306  -0.20124611  1.0062306   0.20124611]\n"," [-0.5003124  -0.5003124   0.5003124   0.5003124 ]], shape=(6, 4), dtype=float32)\n"]},{"data":{"text/plain":["<tf.Tensor: shape=(204600, 4), dtype=float32, numpy=\n","array([[0.        , 0.        , 0.028125  , 0.028125  ],\n","       [0.        , 0.        , 0.02080267, 0.03848034],\n","       [0.        , 0.        , 0.03848034, 0.02080267],\n","       ...,\n","       [0.74875385, 0.        , 1.        , 1.        ],\n","       [0.        , 0.7487539 , 1.        , 1.        ],\n","       [0.4496876 , 0.4496876 , 1.        , 1.        ]], dtype=float32)>"]},"metadata":{},"output_type":"display_data"}],"source":["SSD = {\n","    \"mobilenet_v2\": {\n","        \"img_size\": 640,\n","        \"feature_map_shapes\": [160,80,40,20,10],\n","        \"aspect_ratios\": [[1., 2., 1./2., 5., 1./5.],\n","                          [1., 2., 1./2., 5., 1./5.],\n","                          [1., 2., 1./2., 5., 1./5.],\n","                          [1., 2., 1./2., 5., 1./5.],\n","                          [1., 2., 1./2., 5., 1./5.]],\n","        \"use_custom_scale\": False,\n","        \"scale_min\": 0.05,\n","        \"scale_max\": 0.9,\n","        \"scale\": [0.05, 0.1, 0.2, 0.4, 0.7, 1, 1.5],\n","        \"trainable\": True,\n","        \"num_trainable\": None\n","    },\n","}\n","\n","def get_hyper_params(backbone, **kwargs):\n","    \"\"\"Generating hyper params in a dynamic way.\n","    inputs:\n","        **kwargs = any value could be updated in the hyper_params\n","\n","    outputs:\n","        hyper_params = dictionary\n","    \"\"\"\n","    hyper_params = SSD[backbone]\n","    hyper_params[\"detection\"] = \"BiFPN\" # None / \"FPN\" / \"BiFPN\" / \"PAFPN\" / \"NASFPN\"\n","    hyper_params[\"feature_fusion\"] = None # \"concat\" / \"elesum\" / None\n","    hyper_params[\"dataset\"] = 0 # dut, tilda, daffodil, thesis, combined\n","    hyper_params[\"iou_threshold\"] = 0.5\n","    hyper_params[\"neg_pos_ratio\"] = 3 # neg:pos 3:1 ratio\n","    hyper_params[\"loc_loss_alpha\"] = 1 # weight for the localization loss\n","    hyper_params[\"variances\"] = [0.1, 0.1, 0.2, 0.2]\n","    hyper_params[\"use_focal\"] = False\n","    hyper_params[\"alpha\"] = 2.0\n","    hyper_params[\"gamma\"] = 0.25\n","    hyper_params[\"batch_size\"] = 8\n","    hyper_params[\"epochs\"] = 200\n","    hyper_params[\"lr\"] = 1e-05\n","    hyper_params[\"patience\"] = 20\n","    # overwrite any parameters\n","    for key, value in kwargs.items():\n","        if key in hyper_params and value:\n","            hyper_params[key] = value\n","\n","    return hyper_params\n","\n","hyper_params = get_hyper_params(backbone)\n","display(hyper_params)\n","\n","# We calculate prior boxes for one time and use it for all operations because of the all images are the same sizes\n","prior_boxes = generate_prior_boxes(hyper_params[\"feature_map_shapes\"], hyper_params[\"aspect_ratios\"], hyper_params)\n","display(prior_boxes)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:01:22.573692Z","iopub.status.busy":"2024-02-14T08:01:22.573343Z","iopub.status.idle":"2024-02-14T08:01:22.577867Z","shell.execute_reply":"2024-02-14T08:01:22.576930Z","shell.execute_reply.started":"2024-02-14T08:01:22.573665Z"},"id":"QNLznYpsBG8A","trusted":true},"outputs":[],"source":["# Sanity Check\n","assert isinstance(hyper_params[\"dataset\"], int) and -1 < hyper_params[\"dataset\"] < 5"]},{"cell_type":"markdown","metadata":{"id":"nmkV2S7HGtZ8"},"source":["# MobileNetV2 SSD\n","\n","Specified backbone"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:01:24.039041Z","iopub.status.busy":"2024-02-14T08:01:24.038221Z","iopub.status.idle":"2024-02-14T08:01:24.042657Z","shell.execute_reply":"2024-02-14T08:01:24.041699Z","shell.execute_reply.started":"2024-02-14T08:01:24.039009Z"},"id":"oy32ZRGbcVlA","trusted":true},"outputs":[],"source":["# from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","# base_model = MobileNetV2(include_top=False, input_shape=(1024, 1024, 3))\n","# base_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:01:24.529612Z","iopub.status.busy":"2024-02-14T08:01:24.529172Z","iopub.status.idle":"2024-02-14T08:01:24.536821Z","shell.execute_reply":"2024-02-14T08:01:24.535971Z","shell.execute_reply.started":"2024-02-14T08:01:24.529578Z"},"id":"qLVgx-fklhgd","trusted":true},"outputs":[],"source":["def conv_layer(filter, kernel_size,\n","               layer, strides=1,\n","               padding='same',\n","               activation='linear',\n","               name='conv2d',pool=False,\n","               poolsize=2,poolstride=2,conv=True):\n","    if conv == True:\n","        layer = tf.keras.layers.Conv2D(filters=filter,\n","                                    kernel_size=kernel_size,\n","                                    strides=strides,\n","                                    activation=activation,\n","                                    padding=padding,\n","                                    name=name,\n","                                    kernel_initializer='he_normal')(layer)\n","        layer = tf.keras.layers.BatchNormalization()(layer)\n","        layer = tf.keras.layers.ReLU()(layer)\n","    elif pool == True:\n","        layer=tf.keras.layers.MaxPool2D(pool_size=(poolsize, poolsize),\n","                                        strides=poolstride, padding='same')(layer)\n","    return layer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:04:44.323855Z","iopub.status.busy":"2024-02-14T08:04:44.323363Z","iopub.status.idle":"2024-02-14T08:04:44.339103Z","shell.execute_reply":"2024-02-14T08:04:44.338090Z","shell.execute_reply.started":"2024-02-14T08:04:44.323820Z"},"id":"yApDXEFhKhJb","trusted":true},"outputs":[],"source":["from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.models import Model\n","\n","def get_model(hyper_params):\n","    \"\"\"Generating ssd model for hyper params.\n","    inputs:\n","        hyper_params = dictionary\n","\n","    outputs:\n","        ssd_model = tf.keras.model\n","    \"\"\"\n","    img_size = hyper_params[\"img_size\"]\n","    num_classes = hyper_params[\"total_labels\"]\n","    base_model = MobileNetV2(include_top=False, input_shape=(img_size, img_size, 3))\n","\n","    if hyper_params[\"trainable\"]:\n","        base_model.trainable = True\n","        if hyper_params[\"num_trainable\"] != None:\n","            for layer in base_model.layers[:-hyper_params[\"num_trainable\"]]:\n","                layer.trainable = False\n","    else: base_model.trainable = False\n","\n","    input = base_model.input\n","\n","    zero_conv = base_model.get_layer(\"block_6_expand_relu\").output # 128x128x192\n","    first_conv = base_model.get_layer(\"block_13_expand_relu\").output # 64x64x576\n","    second_conv = base_model.output # 32x32x1280\n","\n","    # first_conv = base_model.get_layer(\"block_13_expand_relu\").output # 19 x 19 x 576\n","    # second_conv = base_model.output # 10 x 10 x 1280\n","\n","    ############################ Extra Feature Layers Start ############################\n","    extra1_1 = conv_layer(256, (1, 1), strides=(1, 1), padding=\"valid\", activation=\"relu\", name=\"extra1_1\", layer=second_conv)\n","    extra1_2 = conv_layer(512, (3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\", name=\"extra1_2\", layer=extra1_1)\n","\n","    extra2_1 = conv_layer(128, (1, 1), strides=(1, 1), padding=\"valid\", activation=\"relu\", name=\"extra2_1\", layer=extra1_2)\n","    extra2_2 = conv_layer(256, (3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\", name=\"extra2_2\", layer=extra2_1)\n","\n","    extra3_1 = conv_layer(128, (1, 1), strides=(1, 1), padding=\"valid\", activation=\"relu\", name=\"extra3_1\", layer=extra2_2)\n","    extra3_2 = conv_layer(256, (3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\", name=\"extra3_2\", layer=extra3_1)\n","\n","    # extra4_1 = conv_layer(128, (1, 1), strides=(1, 1), padding=\"valid\", activation=\"relu\", name=\"extra4_1\", layer=extra3_2)\n","    # extra4_2 = conv_layer(256, (3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\", name=\"extra4_2\", layer=extra4_1)\n","    ############################ Extra Feature Layers End ############################\n","\n","    pred_deltas, pred_labels = get_head_from_outputs(hyper_params, [zero_conv, first_conv, second_conv, extra1_2, extra2_2, extra3_2])\n","    # pred_deltas, pred_labels = get_head_from_outputs(hyper_params, [first_conv, second_conv, extra1_2, extra2_2, extra3_2, extra4_2])\n","    return Model(inputs=input, outputs=[pred_deltas, pred_labels])\n","\n","def init_model(model, img_size):\n","    \"\"\"Initializing model with dummy data for load weights with optimizer state and also graph construction.\n","    inputs:\n","        model = tf.keras.model\n","\n","    \"\"\"\n","    model(tf.random.uniform((1, img_size, img_size, 3)))"]},{"cell_type":"markdown","metadata":{"id":"AFgi_J-uuhEV"},"source":["# FPN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ESEXhixMlfm1"},"outputs":[],"source":["'''\n","FPN model for Keras.\n","\n","# Reference:\n","- [Feature Pyramid Networks for Object Detection](\n","    https://arxiv.org/abs/1612.03144)\n","\n","'''\n","from tensorflow.keras.layers import Conv2D, UpSampling2D, MaxPooling2D\n","\n","class FPN(tf.keras.Model):\n","    def __init__(self, out_channels=256, **kwargs):\n","        '''Feature Pyramid Networks.\n","\n","        inputs:\n","            out_channels (int): the channels of pyramid feature maps.\n","        '''\n","        super(FPN, self).__init__(**kwargs)\n","        self.out_channels = out_channels\n","        self.fpn_c2p2 = Conv2D(out_channels, (1, 1),\n","                              kernel_initializer='he_normal', name='fpn_c2p2')\n","        self.fpn_c3p3 = Conv2D(out_channels, (1, 1),\n","                              kernel_initializer='he_normal', name='fpn_c3p3')\n","        self.fpn_c4p4 = Conv2D(out_channels, (1, 1),\n","                              kernel_initializer='he_normal', name='fpn_c4p4')\n","        self.fpn_c5p5 = Conv2D(out_channels, (1, 1),\n","                              kernel_initializer='he_normal', name='fpn_c5p5')\n","\n","        self.fpn_p3upsampled = UpSampling2D(size=(2, 2), name='fpn_p3upsampled')\n","        self.fpn_p4upsampled = UpSampling2D(size=(2, 2), name='fpn_p4upsampled')\n","        self.fpn_p5upsampled = UpSampling2D(size=(2, 2), name='fpn_p5upsampled')\n","\n","\n","        self.fpn_p2 = Conv2D(out_channels, (3, 3), padding=\"same\",\n","                                    kernel_initializer='he_normal', name='fpn_p2')\n","        self.fpn_p3 = Conv2D(out_channels, (3, 3), padding=\"same\",\n","                                    kernel_initializer='he_normal', name='fpn_p3')\n","        self.fpn_p4 = Conv2D(out_channels, (3, 3), padding=\"same\",\n","                                    kernel_initializer='he_normal', name='fpn_p4')\n","        self.fpn_p5 = Conv2D(out_channels, (3, 3), padding=\"same\",\n","                                    kernel_initializer='he_normal', name='fpn_p5')\n","\n","        self.fpn_p6 = MaxPooling2D(pool_size=(1, 1), strides=2, name='fpn_p6')\n","\n","    def call(self, inputs, training=True):\n","        C2, C3, C4, C5 = inputs\n","\n","        P5 = self.fpn_c5p5(C5)\n","        P4 = self.fpn_c4p4(C4) + self.fpn_p5upsampled(P5)\n","        P3 = self.fpn_c3p3(C3) + self.fpn_p4upsampled(P4)\n","        P2 = self.fpn_c2p2(C2) + self.fpn_p3upsampled(P3)\n","\n","        # Attach 3x3 conv to all P layers to get the final feature maps.\n","        P2 = self.fpn_p2(P2)\n","        P3 = self.fpn_p3(P3)\n","        P4 = self.fpn_p4(P4)\n","        P5 = self.fpn_p5(P5)\n","\n","        # Subsampling from P5 with stride of 2.\n","        P6 = self.fpn_p6(P5)\n","\n","        return [P2, P3, P4, P5, P6]\n","\n","    def compute_output_shape(self, input_shape):\n","        C2_shape, C3_shape, C4_shape, C5_shape = input_shape\n","\n","        C2_shape, C3_shape, C4_shape, C5_shape = \\\n","            C2_shape.as_list(), C3_shape.as_list(), C4_shape.as_list(), C5_shape.as_list()\n","\n","        C6_shape = [C5_shape[0], (C5_shape[1] + 1) // 2, (C5_shape[2] + 1) // 2, self.out_channels]\n","\n","        C2_shape[-1] = self.out_channels\n","        C3_shape[-1] = self.out_channels\n","        C4_shape[-1] = self.out_channels\n","        C5_shape[-1] = self.out_channels\n","\n","        return [tf.TensorShape(C2_shape),\n","                tf.TensorShape(C3_shape),\n","                tf.TensorShape(C4_shape),\n","                tf.TensorShape(C5_shape),\n","                tf.TensorShape(C6_shape)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10935,"status":"ok","timestamp":1708752687336,"user":{"displayName":"happy duck","userId":"12590459388487510630"},"user_tz":-480},"id":"Sw6OJeIRlmNr","outputId":"92e1393d-5a85-4c6f-8966-5916e6a02404"},"outputs":[{"name":"stdout","output_type":"stream","text":["P2 shape: [32, 160, 160, 256]\n","P3 shape: [32, 80, 80, 256]\n","P4 shape: [32, 40, 40, 256]\n","P5 shape: [32, 20, 20, 256]\n","P6 shape: [32, 10, 10, 256]\n"]}],"source":["C2 = tf.random.normal((32, 160, 160, 123))\n","C3 = tf.random.normal((32, 80, 80, 123))\n","C4 = tf.random.normal((32, 40, 40, 123))\n","C5 = tf.random.normal((32, 20, 20, 123))\n","\n","fpn = FPN()\n","\n","P2, P3, P4, P5, P6 = fpn([C2, C3, C4, C5])\n","\n","print('P2 shape:', P2.shape.as_list())\n","print('P3 shape:', P3.shape.as_list())\n","print('P4 shape:', P4.shape.as_list())\n","print('P5 shape:', P5.shape.as_list())\n","print('P6 shape:', P6.shape.as_list())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2JOpD1uWlpuI"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.models import Model\n","\n","def get_fpnmodel(hyper_params):\n","    \"\"\"Generating ssd model for hyper params.\n","    inputs:\n","        hyper_params = dictionary\n","\n","    outputs:\n","        ssd_model = tf.keras.model\n","    \"\"\"\n","    img_size = hyper_params[\"img_size\"]\n","    base_model = MobileNetV2(include_top=False, input_shape=(img_size, img_size, 3))\n","    base_model.trainable = True\n","\n","    input = base_model.input\n","\n","    C2 = base_model.get_layer(\"block_3_expand_relu\").output\n","    C3 = base_model.get_layer(\"block_6_expand_relu\").output\n","    C4 = base_model.get_layer(\"block_13_expand_relu\").output\n","    C5 = base_model.output\n","\n","    fpn = FPN()\n","\n","    P2, P3, P4, P5, P6 = fpn([C2, C3, C4, C5])\n","\n","    pred_deltas, pred_labels = get_head_from_outputs(hyper_params, [P2, P3, P4, P5, P6])\n","    return Model(inputs=input, outputs=[pred_deltas, pred_labels])\n","\n","def init_model(model, img_size=640):\n","    \"\"\"Initializing model with dummy data for load weights with optimizer state and also graph construction.\n","    inputs:\n","        model = tf.keras.model\n","\n","    \"\"\"\n","    model(tf.random.uniform((1, img_size, img_size, 3)))"]},{"cell_type":"markdown","metadata":{"id":"SXQ8MsFpumBk"},"source":["# PAFPN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgWHUYsslqTJ"},"outputs":[],"source":["'''\n","PAFPN model for Keras.\n","'''\n","from tensorflow.keras.layers import Conv2D, UpSampling2D, MaxPooling2D, Concatenate\n","\n","class PAFPN(tf.keras.Model):\n","    def __init__(self, out_channels=256, **kwargs):\n","        super(PAFPN, self).__init__(**kwargs)\n","        self.out_channels = out_channels\n","        self.fpn_c2p2 = Conv2D(out_channels, (1, 1),\n","                              kernel_initializer='he_normal', name='fpn_c2p2')\n","        self.fpn_c3p3 = Conv2D(out_channels, (1, 1),\n","                              kernel_initializer='he_normal', name='fpn_c3p3')\n","        self.fpn_c4p4 = Conv2D(out_channels, (1, 1),\n","                              kernel_initializer='he_normal', name='fpn_c4p4')\n","        self.fpn_c5p5 = Conv2D(out_channels, (1, 1),\n","                              kernel_initializer='he_normal', name='fpn_c5p5')\n","\n","        self.fpn_p3upsampled = UpSampling2D(size=(2, 2), name='fpn_p3upsampled')\n","        self.fpn_p4upsampled = UpSampling2D(size=(2, 2), name='fpn_p4upsampled')\n","        self.fpn_p5upsampled = UpSampling2D(size=(2, 2), name='fpn_p5upsampled')\n","\n","        self.fpn_p2 = Conv2D(out_channels, (3, 3), padding=\"same\",\n","                                    kernel_initializer='he_normal', name='fpn_p2')\n","        self.fpn_p3 = Conv2D(out_channels, (3, 3), padding=\"same\",\n","                                    kernel_initializer='he_normal', name='fpn_p3')\n","        self.fpn_p4 = Conv2D(out_channels, (3, 3), padding=\"same\",\n","                                    kernel_initializer='he_normal', name='fpn_p4')\n","        self.fpn_p5 = Conv2D(out_channels, (3, 3), padding=\"same\",\n","                                    kernel_initializer='he_normal', name='fpn_p5')\n","\n","        self.pafpn_n2downsampled = MaxPooling2D(pool_size=(2, 2), name='pafpn_n2downsampled')\n","        self.pafpn_n3downsampled = MaxPooling2D(pool_size=(2, 2), name='pafpn_n3downsampled')\n","        self.pafpn_n4downsampled = MaxPooling2D(pool_size=(2, 2), name='pafpn_n4downsampled')\n","\n","        self.pafpn_p2p3 = Concatenate(axis=-1)\n","        self.pafpn_p3p4 = Concatenate(axis=-1)\n","        self.pafpn_p4p5 = Concatenate(axis=-1)\n","\n","        self.pafpn_p2n2 = Conv2D(out_channels, (1, 1),\n","                              kernel_initializer='he_normal', name='pafpn_p2n2')\n","        self.pafpn_n2 = Conv2D(out_channels, (3, 3), padding=\"same\",\n","                                    kernel_initializer='he_normal', name='pafpn_n2')\n","        self.pafpn_p3n3 = Conv2D(out_channels, (1, 1),\n","                              kernel_initializer='he_normal', name='pafpn_p3n3')\n","        self.pafpn_n3 = Conv2D(out_channels, (3, 3), padding=\"same\",\n","                                    kernel_initializer='he_normal', name='pafpn_n3')\n","        self.pafpn_p4n4 = Conv2D(out_channels, (1, 1),\n","                              kernel_initializer='he_normal', name='pafpn_p4n4')\n","        self.pafpn_n4 = Conv2D(out_channels, (3, 3), padding=\"same\",\n","                                    kernel_initializer='he_normal', name='pafpn_n4')\n","        self.pafpn_p5n5 = Conv2D(out_channels, (1, 1),\n","                              kernel_initializer='he_normal', name='pafpn_p5n5')\n","        self.pafpn_n5 = Conv2D(out_channels, (3, 3), padding=\"same\",\n","                                    kernel_initializer='he_normal', name='pafpn_n5')\n","\n","    def call(self, inputs, training=True):\n","        C2, C3, C4, C5 = inputs\n","\n","        P5 = self.fpn_c5p5(C5)\n","        P4 = self.fpn_c4p4(C4) + self.fpn_p5upsampled(P5)\n","        P3 = self.fpn_c3p3(C3) + self.fpn_p4upsampled(P4)\n","        P2 = self.fpn_c2p2(C2) + self.fpn_p3upsampled(P3)\n","\n","        # Attach 3x3 conv to all P layers to get the final feature maps.\n","        P2 = self.fpn_p2(P2)\n","        P3 = self.fpn_p3(P3)\n","        P4 = self.fpn_p4(P4)\n","        P5 = self.fpn_p5(P5)\n","\n","        N2 = self.pafpn_p2n2(P2)\n","        P3 = self.pafpn_p2p3([self.pafpn_n2downsampled(N2), P3])\n","        N3 = self.pafpn_p3n3(P3)\n","        P4 = self.pafpn_p3p4([self.pafpn_n3downsampled(N3), P4])\n","        N4 = self.pafpn_p4n4(P4)\n","        P5 = self.pafpn_p4p5([self.pafpn_n4downsampled(N4), P5])\n","        N5 = self.pafpn_p5n5(P5)\n","\n","        N2 = self.pafpn_n2(N2)\n","        N3 = self.pafpn_n3(N3)\n","        N4 = self.pafpn_n4(N4)\n","        N5 = self.pafpn_n5(N5)\n","\n","        return [N2, N3, N4, N5]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1708752687339,"user":{"displayName":"happy duck","userId":"12590459388487510630"},"user_tz":-480},"id":"Z-ANUACiDMjx","outputId":"4845767f-4c14-4a57-b07a-ee2083185c64"},"outputs":[{"data":{"text/plain":["TensorShape([2, 32, 32, 256])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["ts = tf.random.normal((2, 64, 64, 256))\n","pooled_ts = MaxPooling2D(pool_size=(2, 2), padding='same')(ts)\n","pooled_ts.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1708753725027,"user":{"displayName":"happy duck","userId":"12590459388487510630"},"user_tz":-480},"id":"yW2m4IItGhwy","outputId":"220edb6f-89f1-448d-fe74-91ac88afb9b8"},"outputs":[{"data":{"text/plain":["TensorShape([2, 64, 64, 512])"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["ts1 = tf.random.normal((2, 64, 64, 256))\n","ts2 = tf.random.normal((2, 64, 64, 256))\n","concat_ts = Concatenate(axis=-1)([ts1, ts2])\n","concat_ts.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1708752687341,"user":{"displayName":"happy duck","userId":"12590459388487510630"},"user_tz":-480},"id":"N5fUUwDrIIdg","outputId":"e36d6c32-e7b2-4abc-b263-e3ea30364188"},"outputs":[{"data":{"text/plain":["TensorShape([2, 64, 64, 256])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["ts = tf.random.normal((2, 64, 64, 256))\n","conv_ts = Conv2D(256, (3, 3), padding=\"same\", kernel_initializer='he_normal')(ts)\n","conv_ts.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":746,"status":"ok","timestamp":1708752688069,"user":{"displayName":"happy duck","userId":"12590459388487510630"},"user_tz":-480},"id":"VQgqvdgxlr9K","outputId":"14c8edcc-000b-4ff5-d660-df6abae11166"},"outputs":[{"name":"stdout","output_type":"stream","text":["P2 shape: [32, 160, 160, 256]\n","P3 shape: [32, 80, 80, 256]\n","P4 shape: [32, 40, 40, 256]\n","P5 shape: [32, 20, 20, 256]\n"]}],"source":["C2 = tf.random.normal((32, 160, 160, 144))\n","C3 = tf.random.normal((32, 80, 80, 192))\n","C4 = tf.random.normal((32, 40, 40, 576))\n","C5 = tf.random.normal((32, 20, 20, 1280))\n","\n","pafpn = PAFPN()\n","\n","N2, N3, N4, N5 = pafpn([C2, C3, C4, C5])\n","\n","print('P2 shape:', N2.shape.as_list())\n","print('P3 shape:', N3.shape.as_list())\n","print('P4 shape:', N4.shape.as_list())\n","print('P5 shape:', N5.shape.as_list())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmTVx5qnluEx"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.models import Model\n","\n","def get_pafpnmodel(hyper_params):\n","    \"\"\"Generating ssd model for hyper params.\n","    inputs:\n","        hyper_params = dictionary\n","\n","    outputs:\n","        ssd_model = tf.keras.model\n","    \"\"\"\n","    img_size = hyper_params[\"img_size\"]\n","    base_model = MobileNetV2(include_top=False, input_shape=(img_size, img_size, 3))\n","    base_model.trainable = True\n","\n","    input = base_model.input\n","\n","    C2 = base_model.get_layer(\"block_3_expand_relu\").output\n","    C3 = base_model.get_layer(\"block_6_expand_relu\").output\n","    C4 = base_model.get_layer(\"block_13_expand_relu\").output\n","    C5 = base_model.output\n","\n","    pafpn = PAFPN()\n","\n","    N2, N3, N4, N5 = pafpn([C2, C3, C4, C5])\n","\n","    pred_deltas, pred_labels = get_head_from_outputs(hyper_params, [N2, N3, N4, N5])\n","    return Model(inputs=input, outputs=[pred_deltas, pred_labels])\n","\n","def init_model(model, img_size=640):\n","    \"\"\"Initializing model with dummy data for load weights with optimizer state and also graph construction.\n","    inputs:\n","        model = tf.keras.model\n","\n","    \"\"\"\n","    model(tf.random.uniform((1, img_size, img_size, 3)))"]},{"cell_type":"markdown","metadata":{"id":"7tu_b2OJupEU"},"source":["# BiFPN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IsY8AQLFuIL5"},"outputs":[],"source":["\"\"\"Implementations of layers/models used in EfficientDet.\"\"\"\n","\n","import numpy as np\n","\n","\n","class BiFPNLayerNode(tf.keras.layers.Layer):\n","    \"\"\"One node in BiFPN for features fusing.\"\"\"\n","\n","    def __init__(self,\n","                 channels=64,\n","                 kernel_size=3,\n","                 depth_multiplier=1,\n","                 name='BiFPN_node'):\n","        \"\"\"Ininitialize node.\n","\n","        Args:\n","            channels: an integer representing number of units inside the node.\n","            kernel_size: an integer or tuple/list of 2 integers, specifying\n","                the height and width of the 2D convolution window.\n","            depth_multiplier: an integer representing depth multiplier for\n","                separable convolution layer.\n","            name: a string representing layer name.\n","        \"\"\"\n","        super().__init__(name=name)\n","        self.channels = channels\n","        self.depth_multiplier = depth_multiplier\n","        self.kernel_size = kernel_size\n","\n","    def build(self, inputs):\n","        self.w = self.add_weight(\n","            shape=(len(inputs), self.channels),\n","            initializer=\"ones\",\n","            name='sum_weights',\n","            trainable=True\n","        )\n","\n","        self.conv2d = tf.keras.layers.SeparableConv2D(\n","            self.channels,\n","            self.kernel_size,\n","            padding='same',\n","            depth_multiplier=self.depth_multiplier,\n","            pointwise_initializer=tf.initializers.variance_scaling(),\n","            depthwise_initializer=tf.initializers.variance_scaling(),\n","            name='node_conv'\n","        )\n","\n","        self.bn = tf.keras.layers.BatchNormalization()\n","        self.act = tf.keras.layers.Activation(tf.nn.silu)\n","\n","    def call(self, inputs, training=False):\n","        \"\"\"Fuse features.\n","\n","        Args:\n","            inputs: a list with length equal to self.w.shape[0] of feature maps\n","                with equal shapes.\n","\n","        Returns:\n","            A float tensor of fused features after applying convolution\n","            with batch normalization and SiLU activation.\n","        \"\"\"\n","        norm = tf.math.reduce_sum(self.w, axis=0) + 1e-4\n","        scaled_tensors = [inputs[i] * self.w[i] / norm for i in range(self.w.shape[0])]\n","        w_sum = tf.math.add_n(scaled_tensors)\n","        conv = self.conv2d(w_sum)\n","        bn = self.bn(conv, training=training)\n","        return self.act(bn)\n","\n","\n","class BiFPNLayer(tf.keras.layers.Layer):\n","    \"\"\"One layer of BiFPN.\"\"\"\n","\n","    def __init__(self,\n","                 channels=64,\n","                 kernel_size=3,\n","                 depth_multiplier=1,\n","                 pooling_strategy='avg',\n","                 name='BiFPN_Layer'):\n","        \"\"\"Initialize BiFPN layer.\n","\n","        Args:\n","            channels: an integer representing number of units inside each fusing node.\n","            kernel_size: an integer or tuple/list of 2 integers, specifying\n","                the height and width of the 2D convolution window.\n","            depth_multiplier: an integer representing depth multiplier for\n","                separable convolution layers in BiFPN nodes.\n","            pooling_strategy: a string representing pooling strategy.\n","                'avg' or 'max'. Otherwise the max pooling will be selected.\n","            name: a string representing layer name.\n","        \"\"\"\n","        super().__init__(name=name)\n","        self.pooling_strategy = pooling_strategy\n","\n","        self.first_step_nodes = [BiFPNLayerNode(channels=channels,\n","                                                kernel_size=kernel_size,\n","                                                depth_multiplier=depth_multiplier,\n","                                                name=f'step_1_level_{i}_node') for i in range(4, 7)]\n","        self.second_step_nodes = [BiFPNLayerNode(channels=channels,\n","                                                 kernel_size=kernel_size,\n","                                                 depth_multiplier=depth_multiplier,\n","                                                 name=f'step_2_level_{i}_node') for i in range(3, 8)]\n","\n","    def call(self, inputs, training=False):\n","        \"\"\"Perfrom features fusing from different levels.\"\"\"\n","\n","        upscaled = self._upscale2d(inputs[-1])\n","        first_step_outs = [self.first_step_nodes[-1]([inputs[-2], upscaled], training=training)]\n","        for i in range(2):\n","            upscaled = self._upscale2d(first_step_outs[i])\n","            fused = self.first_step_nodes[1-i]([inputs[-3-i], upscaled])\n","            first_step_outs.append(fused)\n","\n","        upscaled = self._upscale2d(first_step_outs[-1])\n","        second_step_outs = [self.second_step_nodes[0]([inputs[0], upscaled])]\n","        for i in range(1, 4):\n","            downscaled = self._pool2d(second_step_outs[-1])\n","            fused = self.second_step_nodes[i]([inputs[i], first_step_outs[3-i], downscaled], training=training)\n","            second_step_outs.append(fused)\n","        downscaled = self._pool2d(second_step_outs[-1])\n","        fused = self.second_step_nodes[-1]([inputs[-1], downscaled])\n","        second_step_outs.append(fused)\n","\n","        return second_step_outs\n","\n","    def _pool2d(self, inputs):\n","        #TODO: Optimize calling of pooling layer.\n","        if self.pooling_strategy == 'avg':\n","            return tf.keras.layers.AveragePooling2D()(inputs)\n","        else:\n","            return tf.keras.layers.MaxPool2D()(inputs)\n","\n","    def _upscale2d(self, inputs):\n","        return tf.keras.layers.UpSampling2D()(inputs)\n","\n","\n","class BiFPN(tf.keras.layers.Layer):\n","    \"\"\"Bidirectional Feature Pyramid Network.\"\"\"\n","\n","    def __init__(self,\n","                 channels=64,\n","                 depth=3,\n","                 kernel_size=3,\n","                 depth_multiplier=1,\n","                 pooling_strategy='avg',\n","                 name='BiFPN'):\n","        super().__init__(name=name)\n","        \"\"\"Initialize BiFPN.\n","\n","        Args:\n","            channels: an integer representing number of units inside each fusing node\n","                and convolution layer.\n","            depth: an integer representing number of BiFPN layers. depth > 0.\n","            kernel_size: an integer or tuple/list of 2 integers, specifying\n","                the height and width of the 2D convolution window.\n","            depth_multiplier: an integer representing depth multiplier for\n","                separable convolution layers in BiFPN nodes.\n","            pooling_strategy: a string representing pooling strategy in BiFPN layers.\n","                'avg' or 'max'. Otherwise the max pooling will be selected.\n","            name: a string representing layer name.\n","        \"\"\"\n","        self.depth = depth\n","        self.channels = channels\n","        self.pooling_strategy = pooling_strategy\n","\n","        self.convs_1x1 = [tf.keras.layers.Conv2D(channels,\n","                                                 1,\n","                                                 padding='same',\n","                                                 name=f'1x1_conv_level_{3+i}') for i in range(5)]\n","\n","        self.bns = [\n","            tf.keras.layers.BatchNormalization(name=f'bn_level_{i}') for i in range(5)\n","        ]\n","        self.act = tf.keras.layers.Activation(tf.nn.silu)\n","\n","        self.bifpn_layers = [BiFPNLayer(channels=channels,\n","                                        kernel_size=kernel_size,\n","                                        depth_multiplier=depth_multiplier,\n","                                        pooling_strategy=pooling_strategy,\n","                                        name=f'BiFPN_Layer_{i}') for i in range(depth)]\n","\n","    def call(self, inputs, training=False):\n","        assert len(inputs) == 5\n","\n","        squeezed = [self.convs_1x1[i](inputs[i]) for i in range(5)]\n","        normalized = [self.bns[i](squeezed[i], training=training) for i in range(5)]\n","        activated = [self.act(normalized[i]) for i in range(5)]\n","        feature_maps = self.bifpn_layers[0](activated, training=training)\n","        for layer in self.bifpn_layers[1:]:\n","            feature_maps = layer(feature_maps, training=training)\n","\n","        return feature_maps\n","\n","\n","class ClassDetector(tf.keras.layers.Layer):\n","    \"\"\"Classification head.\"\"\"\n","\n","    def __init__(self,\n","                 num_classes=80,\n","                 channels=64,\n","                 num_anchors=9,\n","                 depth=3,\n","                 kernel_size=3,\n","                 depth_multiplier=1,\n","                 name='class_det'):\n","        \"\"\"Initialize classification model.\n","\n","        Args:\n","            num_classes: an integer representing number of classes\n","                to predict.\n","            channels: an integer representing number of filters\n","                inside each separable convolution layer.\n","            num_anchors: an integer representing number of anchor\n","                boxes.\n","            depth: an integer representing number of separable\n","                convolutions before final convolution.\n","            kernel_size: an integer or tuple/list of 2 integers, specifying\n","                the height and width of the 2D convolution window.\n","            depth_multiplier: an integer representing depth multiplier for\n","                separable convolution layers.\n","            name: a string representing layer name.\n","        \"\"\"\n","        super().__init__(name=name)\n","        self.num_classes = num_classes\n","        self.channels = channels\n","        self.depth = depth\n","        self.kernel_size = kernel_size\n","        self.depth_multiplier = depth_multiplier\n","\n","        self.convs = [tf.keras.layers.SeparableConv2D(\n","            channels,\n","            kernel_size,\n","            padding='same',\n","            depth_multiplier=depth_multiplier,\n","            pointwise_initializer=tf.initializers.variance_scaling(),\n","            depthwise_initializer=tf.initializers.variance_scaling(),\n","            bias_initializer=tf.zeros_initializer(),\n","            name=f'class_det_separable_conv_{i}'\n","        ) for i in range(depth)]\n","\n","        self.bns = [\n","            tf.keras.layers.BatchNormalization(name=f'bn_{i}') for i in range(depth)\n","        ]\n","        self.act = tf.keras.layers.Activation(tf.nn.silu)\n","\n","        bias_init = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n","        self.classes = tf.keras.layers.SeparableConv2D(\n","            num_classes * num_anchors,\n","            kernel_size,\n","            padding='same',\n","            depth_multiplier=depth_multiplier,\n","            activation=None,\n","            pointwise_initializer=tf.initializers.variance_scaling(),\n","            depthwise_initializer=tf.initializers.variance_scaling(),\n","            bias_initializer=bias_init,\n","            name='class_preds'\n","        )\n","\n","    def call(self, inputs, training=False):\n","        for i in range(self.depth):\n","            inputs = self.convs[i](inputs)\n","            inputs = self.bns[i](inputs, training=training)\n","            inputs = self.act(inputs)\n","        class_output = self.classes(inputs)\n","\n","        return class_output\n","\n","\n","class BoxRegressor(tf.keras.layers.Layer):\n","    \"\"\"Regression head.\"\"\"\n","\n","    def __init__(self,\n","                 channels=64,\n","                 num_anchors=9,\n","                 depth=3,\n","                 kernel_size=3,\n","                 depth_multiplier=1,\n","                 name='box_regressor'):\n","        \"\"\"Initialize regression model.\n","\n","        Args:\n","            channels: an integer representing number of filters\n","                inside each separable convolution layer.\n","            num_anchors: an integer representing number of anchor\n","                boxes.\n","            depth: an integer representing number of separable\n","                convolutions before final convolution.\n","            kernel_size: an integer or tuple/list of 2 integers, specifying\n","                the height and width of the 2D convolution window.\n","            depth_multiplier: an integer representing depth multiplier for\n","                separable convolution layers.\n","            name: a string representing layer name.\n","        \"\"\"\n","        super().__init__(name=name)\n","        self.channels=channels\n","        self.num_anchors=num_anchors\n","        self.depth=depth\n","        self.kernel_size=kernel_size\n","        self.depth_multiplier=depth_multiplier\n","\n","        self.convs = [tf.keras.layers.SeparableConv2D(\n","            channels,\n","            kernel_size,\n","            padding='same',\n","            depth_multiplier=depth_multiplier,\n","            pointwise_initializer=tf.initializers.variance_scaling(),\n","            depthwise_initializer=tf.initializers.variance_scaling(),\n","            bias_initializer=tf.zeros_initializer(),\n","            name=f'box_reg_separable_conv_{i}'\n","        ) for i in range(depth)]\n","\n","        self.bns = [\n","            tf.keras.layers.BatchNormalization(name=f'bn_{i}') for i in range(depth)\n","        ]\n","        self.act = tf.keras.layers.Activation(tf.nn.silu)\n","\n","        self.boxes = tf.keras.layers.SeparableConv2D(\n","            4 * num_anchors,\n","            kernel_size,\n","            padding='same',\n","            depth_multiplier=depth_multiplier,\n","            activation=None,\n","            pointwise_initializer=tf.initializers.variance_scaling(),\n","            depthwise_initializer=tf.initializers.variance_scaling(),\n","            bias_initializer=tf.zeros_initializer(),\n","            name='box_preds'\n","        )\n","\n","    def call(self, inputs, training=False):\n","        for i in range(self.depth):\n","            inputs = self.convs[i](inputs)\n","            inputs = self.bns[i](inputs, training=training)\n","            inputs = self.act(inputs)\n","        box_output = self.boxes(inputs)\n","\n","        return box_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SQx3DQ7u784"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","from tensorflow.keras.layers import Conv2D, AveragePooling2D\n","from tensorflow.keras.models import Model\n","\n","def get_bifpnmodel(hyper_params, channels=64,\n","                 bifpn_depth=3,\n","                 bifpn_kernel_size=3,\n","                 bifpn_depth_multiplier=1,\n","                 bifpn_pooling_strategy='avg',\n","                 num_anchors=6, #9\n","                 heads_depth=3,\n","                 class_kernel_size=3,\n","                 class_depth_multiplier=1,\n","                 box_kernel_size=3,\n","                 box_depth_multiplier=1):\n","\n","    img_size = hyper_params[\"img_size\"]\n","    num_classes = hyper_params[\"total_labels\"]\n","    batch_size = hyper_params[\"batch_size\"]\n","    base_model = MobileNetV2(include_top=False, input_shape=(img_size, img_size, 3))\n","    base_model.trainable = True\n","\n","    input = base_model.input\n","\n","    C2 = base_model.get_layer(\"block_3_expand_relu\").output\n","    C3 = base_model.get_layer(\"block_6_expand_relu\").output\n","    C4 = base_model.get_layer(\"block_13_expand_relu\").output\n","    C5 = base_model.output\n","    C6 = AveragePooling2D(pool_size=(2, 2), padding='same')(C5)\n","\n","    bifpn = BiFPN(channels=channels,\n","                  depth=bifpn_depth,\n","                  kernel_size=bifpn_kernel_size,\n","                  depth_multiplier=bifpn_depth_multiplier,\n","                  pooling_strategy=bifpn_pooling_strategy)\n","\n","    bifpn_features = bifpn([C2,C3,C4,C5,C6], training=False)\n","\n","    # class_det = ClassDetector(channels=channels,\n","    #                           num_classes=num_classes,\n","    #                           num_anchors=num_anchors,\n","    #                           depth=heads_depth,\n","    #                           kernel_size=class_kernel_size,\n","    #                           depth_multiplier=class_depth_multiplier)\n","    # box_reg = BoxRegressor(channels=channels,\n","    #                        num_anchors=num_anchors,\n","    #                        depth=heads_depth,\n","    #                        kernel_size=box_kernel_size,\n","    #                        depth_multiplier=box_depth_multiplier)\n","    # labels_head = []\n","    # boxes_head = []\n","    # for feature in bifpn_features:\n","    #     labels_head.append(class_det(feature, training=False))\n","    #     boxes_head.append(box_reg(feature, training=False))\n","    # pred_labels = HeadWrapper(num_classes, name=\"labels_head\")(labels_head)\n","    # pred_labels = Activation(\"softmax\", name=\"conf\")(pred_labels)\n","    # pred_deltas = HeadWrapper(4, name=\"loc\")(boxes_head)\n","\n","    pred_deltas, pred_labels = get_head_from_outputs(hyper_params, bifpn_features)\n","    return Model(inputs=input, outputs=[pred_deltas, pred_labels])\n","\n","def init_model(model, img_size=640):\n","    \"\"\"Initializing model with dummy data for load weights with optimizer state and also graph construction.\n","    inputs:\n","        model = tf.keras.model\n","\n","    \"\"\"\n","    model(tf.random.uniform((1, img_size, img_size, 3)))"]},{"cell_type":"markdown","metadata":{"id":"iG7-Fv5juqdE"},"source":["# NASFPN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sQg6v3Aurm2"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","from __future__ import absolute_import, division, print_function\n","import tensorflow as tf\n","import tensorflow.contrib.slim as slim\n","from libs.configs import cfgs\n","from libs.networks.resnet import fusion_two_layer\n","\n","\n","def fpn(feature_dict, scope):\n","    pyramid_dict = {}\n","    with tf.variable_scope('build_pyramid_{}'.format(scope)):\n","        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n","                            activation_fn=None, normalizer_fn=None):\n","\n","            P5 = slim.conv2d(feature_dict['P5'],\n","                             num_outputs=cfgs.FPN_CHANNEL,\n","                             kernel_size=[1, 1],\n","                             stride=1, scope='build_P5')\n","\n","            pyramid_dict['P5'] = P5\n","\n","            for level in range(4, 1, -1):  # build [P4, P3, P2]\n","\n","                pyramid_dict['P%d' % level] = fusion_two_layer(C_i=feature_dict[\"P%d\" % level],\n","                                                               P_j=pyramid_dict[\"P%d\" % (level + 1)],\n","                                                               scope='build_P%d' % level)\n","            for level in range(5, 1, -1):\n","                pyramid_dict['P%d' % level] = slim.conv2d(pyramid_dict['P%d' % level],\n","                                                          num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3],\n","                                                          padding=\"SAME\", stride=1, scope=\"fuse_P%d\" % level,\n","                                                          activation_fn=tf.nn.relu if cfgs.USE_RELU else None)\n","            if \"P6\" in cfgs.LEVLES:\n","                P6 = slim.avg_pool2d(P5, kernel_size=[1, 1], stride=2, scope='build_P6')\n","                pyramid_dict['P6'] = P6\n","            return pyramid_dict\n","\n","\n","def gp(fm1, fm2, scope):\n","    h, w = tf.shape(fm2)[1], tf.shape(fm2)[2]\n","    global_ctx = tf.reduce_mean(fm1, axis=[1, 2], keep_dims=True)\n","    global_ctx = tf.sigmoid(global_ctx)\n","    output = (global_ctx * fm2) + tf.image.resize_bilinear(fm1, size=[h, w], name='resize_' + scope)\n","    return output\n","\n","\n","def rcb(fm, scope):\n","    fm = tf.nn.relu(fm)\n","    fm = slim.conv2d(fm, num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3],\n","                     padding=\"SAME\", stride=1, scope='RCB_%s' % scope,\n","                     activation_fn=None)\n","    fm = slim.batch_norm(fm, scope='BN_%s' % scope)\n","    return fm\n","\n","\n","def sum_fm(fm1, fm2, scope):\n","    h, w = tf.shape(fm2)[1], tf.shape(fm2)[2]\n","    output = fm2 + tf.image.resize_bilinear(fm1, size=[h, w], name='resize_' + scope)\n","    return output\n","\n","\n","def nas_fpn(feature_dict, scope):\n","    GP_P5_P3 = gp(feature_dict['P5'], feature_dict['P3'], 'GP_P5_P3_{}'.format(scope))\n","    GP_P5_P3_RCB = rcb(GP_P5_P3, 'GP_P5_P3_RCB_{}'.format(scope))\n","    SUM1 = sum_fm(GP_P5_P3_RCB, feature_dict['P3'], 'SUM1_{}'.format(scope))\n","    SUM1_RCB = rcb(SUM1, 'SUM1_RCB_{}'.format(scope))\n","    SUM2 = sum_fm(SUM1_RCB, feature_dict['P2'], 'SUM2_{}'.format(scope))\n","    SUM2_RCB = rcb(SUM2, 'SUM2_RCB_{}'.format(scope))  # P2\n","    SUM3 = sum_fm(SUM2_RCB, SUM1_RCB, 'SUM3_{}'.format(scope))\n","    SUM3_RCB = rcb(SUM3, 'SUM3_RCB_{}'.format(scope))  # P3\n","    SUM3_RCB_GP = gp(SUM2_RCB, SUM3_RCB, 'SUM3_RCB_GP_{}'.format(scope))\n","    SUM4 = sum_fm(SUM3_RCB_GP, feature_dict['P4'], 'SUM4_{}'.format(scope))\n","    SUM4_RCB = rcb(SUM4, 'SUM4_RCB_{}'.format(scope))  # P4\n","    SUM4_RCB_GP = gp(SUM1_RCB, SUM4_RCB, 'SUM4_RCB_GP_{}'.format(scope))\n","    SUM5 = sum_fm(SUM4_RCB_GP, feature_dict['P6'], 'SUM5_{}'.format(scope))\n","    SUM5_RCB = rcb(SUM5, 'SUM5_RCB_{}'.format(scope))  # P6\n","    h, w = tf.shape(feature_dict['P5'])[1], tf.shape(feature_dict['P5'])[2]\n","    SUM5_RCB_resize = tf.image.resize_bilinear(SUM5_RCB, size=[h, w], name='resize_SUM5_RCB_resize_'.format(scope))\n","    SUM4_RCB_GP1 = gp(SUM4_RCB, SUM5_RCB_resize, 'SUM4_RCB_GP1_{}'.format(scope))\n","    SUM4_RCB_GP1_RCB = rcb(SUM4_RCB_GP1, 'SUM4_RCB_GP1_RCB_{}'.format(scope))  # P5\n","    pyramid_dict = {'P2': SUM2_RCB, 'P3': SUM3_RCB, 'P4': SUM4_RCB,\n","                    'P5': SUM4_RCB_GP1_RCB, 'P6': SUM5_RCB}\n","    return pyramid_dict"]},{"cell_type":"markdown","metadata":{"id":"TTVGVeV8BG8P"},"source":["# Roboflow Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2024-02-14T08:01:33.483247Z","iopub.status.busy":"2024-02-14T08:01:33.482360Z","iopub.status.idle":"2024-02-14T08:01:50.969936Z","shell.execute_reply":"2024-02-14T08:01:50.968921Z","shell.execute_reply.started":"2024-02-14T08:01:33.483212Z"},"executionInfo":{"elapsed":13246,"status":"ok","timestamp":1708752701303,"user":{"displayName":"happy duck","userId":"12590459388487510630"},"user_tz":-480},"id":"vGmfrj73iUhb","outputId":"13ec02f5-cd53-4f4a-a5e8-8d5b41f858ce","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting roboflow\n","  Downloading roboflow-1.1.19-py3-none-any.whl (70 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.2/70.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting certifi==2023.7.22 (from roboflow)\n","  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting chardet==4.0.0 (from roboflow)\n","  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cycler==0.10.0 (from roboflow)\n","  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n","Collecting idna==2.10 (from roboflow)\n","  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.25.2)\n","Collecting opencv-python-headless==4.8.0.74 (from roboflow)\n","  Downloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n","Collecting python-dotenv (from roboflow)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n","Collecting supervision (from roboflow)\n","  Downloading supervision-0.18.0-py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.2)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n","Collecting requests-toolbelt (from roboflow)\n","  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-magic (from roboflow)\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.49.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n","Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (0.7.1)\n","Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (1.11.4)\n","Installing collected packages: python-magic, python-dotenv, opencv-python-headless, idna, cycler, chardet, certifi, supervision, requests-toolbelt, roboflow\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.9.0.80\n","    Uninstalling opencv-python-headless-4.9.0.80:\n","      Successfully uninstalled opencv-python-headless-4.9.0.80\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.6\n","    Uninstalling idna-3.6:\n","      Successfully uninstalled idna-3.6\n","  Attempting uninstall: cycler\n","    Found existing installation: cycler 0.12.1\n","    Uninstalling cycler-0.12.1:\n","      Successfully uninstalled cycler-0.12.1\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: certifi\n","    Found existing installation: certifi 2024.2.2\n","    Uninstalling certifi-2024.2.2:\n","      Successfully uninstalled certifi-2024.2.2\n","Successfully installed certifi-2023.7.22 chardet-4.0.0 cycler-0.10.0 idna-2.10 opencv-python-headless-4.8.0.74 python-dotenv-1.0.1 python-magic-0.4.27 requests-toolbelt-1.0.0 roboflow-1.1.19 supervision-0.18.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["certifi","chardet","cv2","cycler","idna"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip install roboflow\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"YOUR API KEY\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-14T08:01:50.973551Z","iopub.status.busy":"2024-02-14T08:01:50.972465Z","iopub.status.idle":"2024-02-14T08:03:02.209249Z","shell.execute_reply":"2024-02-14T08:03:02.208071Z","shell.execute_reply.started":"2024-02-14T08:01:50.973504Z"},"executionInfo":{"elapsed":5531,"status":"ok","timestamp":1708752706803,"user":{"displayName":"happy duck","userId":"12590459388487510630"},"user_tz":-480},"id":"hszpdXFNv3zP","outputId":"25bbbac2-ab6b-4cb9-b542-e4d83562c3a9","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'models'...\n","remote: Enumerating objects: 4074, done.\u001b[K\n","remote: Counting objects: 100% (4074/4074), done.\u001b[K\n","remote: Compressing objects: 100% (3061/3061), done.\u001b[K\n","remote: Total 4074 (delta 1183), reused 2877 (delta 953), pack-reused 0\u001b[K\n","Receiving objects: 100% (4074/4074), 44.60 MiB | 15.86 MiB/s, done.\n","Resolving deltas: 100% (1183/1183), done.\n"]}],"source":["import os\n","import pathlib\n","\n","# Clone the tensorflow models repository if it doesn't already exist\n","if \"models\" in pathlib.Path.cwd().parts:\n","  while \"models\" in pathlib.Path.cwd().parts:\n","    os.chdir('..')\n","elif not pathlib.Path('models').exists():\n","  !git clone --depth 1 https://github.com/tensorflow/models"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67298,"status":"ok","timestamp":1708752774097,"user":{"displayName":"happy duck","userId":"12590459388487510630"},"user_tz":-480},"id":"qSmRA0scBmGu","outputId":"144f8dd7-3385-4ea6-870f-3f90f9a2038f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing /content/models/research\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Collecting avro-python3 (from object-detection==0.1)\n","  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Collecting apache-beam (from object-detection==0.1)\n","  Downloading apache_beam-2.54.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.8/14.8 MB 60.6 MB/s eta 0:00:00\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (9.4.0)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (4.9.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (3.7.1)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (3.0.8)\n","Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (21.6.0)\n","Requirement already satisfied: tf-slim in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.16.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.0.7)\n","Collecting lvis (from object-detection==0.1)\n","  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.11.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.5.3)\n","Collecting tf-models-official>=2.5.1 (from object-detection==0.1)\n","  Downloading tf_models_official-2.15.0-py2.py3-none-any.whl (2.7 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 90.9 MB/s eta 0:00:00\n","Collecting tensorflow_io (from object-detection==0.1)\n","  Downloading tensorflow_io-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.4 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.4/49.4 MB 12.2 MB/s eta 0:00:00\n","Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.15.0)\n","Collecting pyparsing==2.4.7 (from object-detection==0.1)\n","  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.8/67.8 kB 8.3 MB/s eta 0:00:00\n","Collecting sacrebleu<=2.2.0 (from object-detection==0.1)\n","  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.6/116.6 kB 16.4 MB/s eta 0:00:00\n","Collecting portalocker (from sacrebleu<=2.2.0->object-detection==0.1)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2023.12.25)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (1.25.2)\n","Collecting colorama (from sacrebleu<=2.2.0->object-detection==0.1)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n","Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.84.0)\n","Collecting immutabledict (from tf-models-official>=2.5.1->object-detection==0.1)\n","  Downloading immutabledict-4.1.0-py3-none-any.whl (4.5 kB)\n","Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.16)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.8.0.74)\n","Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.9.5)\n","Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (9.0.0)\n","Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (6.0.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.1.99)\n","Collecting seqeval (from tf-models-official>=2.5.1->object-detection==0.1)\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 5.9 MB/s eta 0:00:00\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.9.4)\n","Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.16.1)\n","Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official>=2.5.1->object-detection==0.1)\n","  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 242.5/242.5 kB 22.7 MB/s eta 0:00:00\n","Collecting tensorflow-text~=2.15.0 (from tf-models-official>=2.5.1->object-detection==0.1)\n","  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 65.2 MB/s eta 0:00:00\n","Requirement already satisfied: tensorflow~=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.15.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object-detection==0.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object-detection==0.1) (2023.4)\n","Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim->object-detection==0.1) (1.4.0)\n","Collecting crcmod<2.0,>=1.7 (from apache-beam->object-detection==0.1)\n","  Downloading crcmod-1.7.tar.gz (89 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.7/89.7 kB 11.4 MB/s eta 0:00:00\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Collecting orjson<4,>=3.9.7 (from apache-beam->object-detection==0.1)\n","  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.5/138.5 kB 9.2 MB/s eta 0:00:00\n","Collecting dill<0.3.2,>=0.3.1.1 (from apache-beam->object-detection==0.1)\n","  Downloading dill-0.3.1.1.tar.gz (151 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.0/152.0 kB 15.0 MB/s eta 0:00:00\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.2.1)\n","Collecting fastavro<2,>=0.23.6 (from apache-beam->object-detection==0.1)\n","  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 27.9 MB/s eta 0:00:00\n","Collecting fasteners<1.0,>=0.3 (from apache-beam->object-detection==0.1)\n","  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n","Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.60.1)\n","Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam->object-detection==0.1)\n","  Downloading hdfs-2.7.3.tar.gz (43 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.5/43.5 kB 3.7 MB/s eta 0:00:00\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.22.0)\n","Collecting js2py<1,>=0.74 (from apache-beam->object-detection==0.1)\n","  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 36.9 MB/s eta 0:00:00\n","Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (4.19.2)\n","Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (3.0.2)\n","Collecting numpy>=1.17 (from sacrebleu<=2.2.0->object-detection==0.1)\n","  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 38.5 MB/s eta 0:00:00\n","Collecting objsize<0.8.0,>=0.6.1 (from apache-beam->object-detection==0.1)\n","  Downloading objsize-0.7.0-py3-none-any.whl (11 kB)\n","Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (23.2)\n","Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam->object-detection==0.1)\n","  Downloading pymongo-4.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 677.2/677.2 kB 36.1 MB/s eta 0:00:00\n","Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.23.0)\n","Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (3.20.3)\n","Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.4.2)\n","Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (4.9.0)\n","Collecting zstandard<1,>=0.18.0 (from apache-beam->object-detection==0.1)\n","  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 73.5 MB/s eta 0:00:00\n","Requirement already satisfied: pyarrow<15.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.6)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (1.4.5)\n","Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (4.8.0.76)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (1.2.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (4.49.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem==0.36.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io->object-detection==0.1) (0.36.0)\n","Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.27.0)\n","Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.1.1)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.11.1)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.1.1)\n","Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam->object-detection==0.1) (5.2)\n","Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam->object-detection==0.1)\n","  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1) (0.33.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1) (0.18.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2023.7.22)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.66.2)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (8.0.4)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.0.7)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (6.1.0)\n","Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam->object-detection==0.1)\n","  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.7/307.7 kB 24.8 MB/s eta 0:00:00\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.10)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (67.7.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (2.4.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (2.15.0)\n","Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.6.0->tf-models-official>=2.5.1->object-detection==0.1) (2.15.0)\n","Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.8)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.5.1)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.3.0)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (4.9)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.2.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (8.1.7)\n","Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.7.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.14.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.10.2)\n","Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (0.42.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2023.6.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (6.1.1)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (3.17.0)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.62.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (5.3.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (3.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (2.1.5)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.2)\n","Building wheels for collected packages: object-detection, avro-python3, crcmod, dill, hdfs, seqeval, pyjsparser, docopt\n","  Building wheel for object-detection (setup.py): started\n","  Building wheel for object-detection (setup.py): finished with status 'done'\n","  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1697356 sha256=67555beae45221b2eed7543c5a63d5985e921deb8d07aa5ea481b5debd91abeb\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-6l56l09z/wheels/53/dd/70/2de274d6c443c69d367bd6a5606f95e5a6df61aacf1435ec0d\n","  Building wheel for avro-python3 (setup.py): started\n","  Building wheel for avro-python3 (setup.py): finished with status 'done'\n","  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=43992 sha256=b1d3524dbcf30e1e1796b9152e8724fcd96cb4908573c0846b7d0c148a595ea7\n","  Stored in directory: /root/.cache/pip/wheels/bc/85/62/6cdd81c56f923946b401cecff38055b94c9b766927f7d8ca82\n","  Building wheel for crcmod (setup.py): started\n","  Building wheel for crcmod (setup.py): finished with status 'done'\n","  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31403 sha256=70e10d8eadd333f3ba0a9b3b97046449a92507b7f1bdcb83a918bbe462190db4\n","  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n","  Building wheel for dill (setup.py): started\n","  Building wheel for dill (setup.py): finished with status 'done'\n","  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78540 sha256=37274a7fff7a6e5efe0700cb659dac4ba2b3cf190e4c5a910ed8665a1cc52881\n","  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n","  Building wheel for hdfs (setup.py): started\n","  Building wheel for hdfs (setup.py): finished with status 'done'\n","  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34325 sha256=ba2f3fd72f28565f83deb60cf14521a6fab79c389a3543ba1548e2dad87ce3ab\n","  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n","  Building wheel for seqeval (setup.py): started\n","  Building wheel for seqeval (setup.py): finished with status 'done'\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=5bab817f47f5437e17d87cd8cff75ec3ab8d0603f7fe44e323365b7ab7774fbb\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","  Building wheel for pyjsparser (setup.py): started\n","  Building wheel for pyjsparser (setup.py): finished with status 'done'\n","  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25984 sha256=487d6c5c7f3d94cc4cb3770264578fc4385c084a58c8280b818e3d083c4b8ce7\n","  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n","  Building wheel for docopt (setup.py): started\n","  Building wheel for docopt (setup.py): finished with status 'done'\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=a4a071aed628850ff3256ecbd34402bb69595cc13980e9514d2e3bd13a99c572\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","Successfully built object-detection avro-python3 crcmod dill hdfs seqeval pyjsparser docopt\n","Installing collected packages: pyjsparser, docopt, crcmod, zstandard, tensorflow_io, pyparsing, portalocker, orjson, objsize, numpy, js2py, immutabledict, fasteners, fastavro, dnspython, dill, colorama, avro-python3, tensorflow-model-optimization, sacrebleu, pymongo, hdfs, seqeval, lvis, apache-beam, tensorflow-text, tf-models-official, object-detection\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.1.1\n","    Uninstalling pyparsing-3.1.1:\n","      Successfully uninstalled pyparsing-3.1.1\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Successfully uninstalled numpy-1.25.2\n","Successfully installed apache-beam-2.54.0 avro-python3-1.10.2 colorama-0.4.6 crcmod-1.7 dill-0.3.1.1 dnspython-2.6.1 docopt-0.6.2 fastavro-1.9.4 fasteners-0.19 hdfs-2.7.3 immutabledict-4.1.0 js2py-0.74 lvis-0.5.3 numpy-1.24.4 object-detection-0.1 objsize-0.7.0 orjson-3.9.15 portalocker-2.8.2 pyjsparser-2.7.1 pymongo-4.6.2 pyparsing-2.4.7 sacrebleu-2.2.0 seqeval-1.2.2 tensorflow-model-optimization-0.8.0 tensorflow-text-2.15.0 tensorflow_io-0.36.0 tf-models-official-2.15.0 zstandard-0.22.0\n"]}],"source":["# Install the Object Detection API\n","%%bash\n","cd models/research/\n","protoc object_detection/protos/*.proto --python_out=.\n","cp object_detection/packages/tf2/setup.py .\n","python -m pip install ."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-14T08:03:02.211882Z","iopub.status.busy":"2024-02-14T08:03:02.211048Z","iopub.status.idle":"2024-02-14T08:03:23.275267Z","shell.execute_reply":"2024-02-14T08:03:23.274347Z","shell.execute_reply.started":"2024-02-14T08:03:02.211836Z"},"executionInfo":{"elapsed":54951,"status":"ok","timestamp":1708752829027,"user":{"displayName":"happy duck","userId":"12590459388487510630"},"user_tz":-480},"id":"b_mYXsDeJMsX","outputId":"73b6d565-66bb-4ddc-a4bb-04266e101834","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["loading Roboflow workspace...\n","loading Roboflow project...\n"]},{"name":"stderr","output_type":"stream","text":["Downloading Dataset Version Zip in Fabric-Defect-Capstone-1 to tfrecord:: 100%|██████████| 789109/789109 [00:43<00:00, 18101.36it/s]"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["\n","Extracting Dataset Version Zip to Fabric-Defect-Capstone-1 in tfrecord:: 100%|██████████| 11/11 [00:02<00:00,  3.77it/s]\n"]}],"source":["if hyper_params[\"dataset\"] == 0:\n","    # dut\n","    project = rf.workspace(\"ducks-zdbul\").project(\"fabric-defect-capstone\")\n","    dataset = project.version(1).download(\"tfrecord\")\n","elif hyper_params[\"dataset\"] == 1:\n","    # tilda\n","    project = rf.workspace(\"irvin-andersen\").project(\"tilda-fabric\")\n","    dataset = project.version(2).download(\"tfrecord\")\n","elif hyper_params[\"dataset\"] == 2:\n","    # daffodil\n","    project = rf.workspace(\"defect-detection-witqu\").project(\"fabric-defect-daffodil\")\n","    dataset = project.version(1).download(\"tfrecord\")\n","elif hyper_params[\"dataset\"] == 3:\n","    # thesis\n","    project = rf.workspace(\"ducks-zdbul\").project(\"fabric-defect-thesis-quv7v\")\n","    dataset = project.version(1).download(\"tfrecord\")\n","elif hyper_params[\"dataset\"] == 4:\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"8N8ZG5fxFYD0"},"source":["# Utils"]},{"cell_type":"markdown","metadata":{"id":"sZFFc8m4HIFH"},"source":["# augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:03:23.279067Z","iopub.status.busy":"2024-02-14T08:03:23.278775Z","iopub.status.idle":"2024-02-14T08:03:23.303951Z","shell.execute_reply":"2024-02-14T08:03:23.303185Z","shell.execute_reply.started":"2024-02-14T08:03:23.279041Z"},"id":"p_NZkMjgHLNp","trusted":true},"outputs":[],"source":["def apply(img, gt_boxes):\n","    \"\"\"Randomly applying data augmentation methods to image and ground truth boxes.\n","    inputs:\n","        img = (height, width, depth)\n","        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","            in normalized form [0, 1]\n","    outputs:\n","        modified_img = (final_height, final_width, depth)\n","        modified_gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","            in normalized form [0, 1]\n","    \"\"\"\n","    # Color operations\n","    # Randomly change hue, saturation, brightness and contrast of image\n","    color_methods = [random_brightness, random_contrast, random_hue, random_saturation]\n","    # Geometric operations\n","    # Randomly sample a patch and flip horizontally image and ground truth boxes\n","    geometric_methods = [patch, flip_horizontally]\n","\n","    for augmentation_method in geometric_methods + color_methods:\n","        img, gt_boxes = randomly_apply_operation(augmentation_method, img, gt_boxes)\n","\n","    img = tf.clip_by_value(img, 0, 1)\n","    return img, gt_boxes\n","\n","def get_random_bool():\n","    \"\"\"Generating random boolean.\n","    outputs:\n","        random boolean 0d tensor\n","    \"\"\"\n","    return tf.greater(tf.random.uniform((), dtype=tf.float32), 0.5)\n","\n","def randomly_apply_operation(operation, img, gt_boxes, *args):\n","    \"\"\"Randomly applying given method to image and ground truth boxes.\n","    inputs:\n","        operation = callable method\n","        img = (height, width, depth)\n","        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","    outputs:\n","        modified_or_not_img = (final_height, final_width, depth)\n","        modified_or_not_gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","    \"\"\"\n","    return tf.cond(\n","        get_random_bool(),\n","        lambda: operation(img, gt_boxes, *args),\n","        lambda: (img, gt_boxes)\n","    )\n","\n","def random_brightness(img, gt_boxes, max_delta=0.12):\n","    \"\"\"Randomly change brightness of the image.\n","    inputs:\n","        img = (height, width, depth)\n","        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","    outputs:\n","        modified_img = (height, width, depth)\n","        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","    \"\"\"\n","    return tf.image.random_brightness(img, max_delta), gt_boxes\n","\n","def random_contrast(img, gt_boxes, lower=0.5, upper=1.5):\n","    \"\"\"Randomly change contrast of the image.\n","    inputs:\n","        img = (height, width, depth)\n","        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","    outputs:\n","        modified_img = (height, width, depth)\n","        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","    \"\"\"\n","    return tf.image.random_contrast(img, lower, upper), gt_boxes\n","\n","def random_hue(img, gt_boxes, max_delta=0.08):\n","    \"\"\"Randomly change hue of the image.\n","    inputs:\n","        img = (height, width, depth)\n","        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","    outputs:\n","        modified_img = (height, width, depth)\n","        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","    \"\"\"\n","    return tf.image.random_hue(img, max_delta), gt_boxes\n","\n","def random_saturation(img, gt_boxes, lower=0.5, upper=1.5):\n","    \"\"\"Randomly change saturation of the image.\n","    inputs:\n","        img = (height, width, depth)\n","        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","    outputs:\n","        modified_img = (height, width, depth)\n","        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","    \"\"\"\n","    return tf.image.random_saturation(img, lower, upper), gt_boxes\n","\n","def flip_horizontally(img, gt_boxes):\n","    \"\"\"Flip image horizontally and adjust the ground truth boxes.\n","    inputs:\n","        img = (height, width, depth)\n","        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","    outputs:\n","        modified_img = (height, width, depth)\n","        modified_gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","    \"\"\"\n","    flipped_img = tf.image.flip_left_right(img)\n","    flipped_gt_boxes = tf.stack([gt_boxes[..., 0],\n","                                1.0 - gt_boxes[..., 3],\n","                                gt_boxes[..., 2],\n","                                1.0 - gt_boxes[..., 1]], -1)\n","    return flipped_img, flipped_gt_boxes\n","\n","##############################################################################\n","## Sample patch start\n","##############################################################################\n","\n","def get_random_min_overlap():\n","    \"\"\"Generating random minimum overlap value.\n","    outputs:\n","        min_overlap = random minimum overlap value 0d tensor\n","    \"\"\"\n","    overlaps = tf.constant([0.1, 0.3, 0.5, 0.7, 0.9], dtype=tf.float32)\n","    i = tf.random.uniform((), minval=0, maxval=tf.shape(overlaps)[0], dtype=tf.int32)\n","    return overlaps[i]\n","\n","def expand_image(img, gt_boxes, height, width):\n","    \"\"\"Randomly expanding image and adjusting ground truth object coordinates.\n","    inputs:\n","        img = (height, width, depth)\n","        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","        height = height of the image\n","        width = width of the image\n","    outputs:\n","        img = (final_height, final_width, depth)\n","        modified_gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","        final_height = final height of the image\n","        final_width = final width of the image\n","    \"\"\"\n","    expansion_ratio = tf.random.uniform((), minval=1, maxval=4, dtype=tf.float32)\n","    final_height, final_width = tf.round(height * expansion_ratio), tf.round(width * expansion_ratio)\n","    pad_left = tf.round(tf.random.uniform((), minval=0, maxval=final_width - width, dtype=tf.float32))\n","    pad_top = tf.round(tf.random.uniform((), minval=0, maxval=final_height - height, dtype=tf.float32))\n","    pad_right = final_width - (width + pad_left)\n","    pad_bottom = final_height - (height + pad_top)\n","\n","    mean, _ = tf.nn.moments(img, [0, 1])\n","    expanded_image = tf.pad(img, ((pad_top, pad_bottom), (pad_left, pad_right), (0,0)), constant_values=-1)\n","    expanded_image = tf.where(expanded_image == -1, mean, expanded_image)\n","\n","    min_max = tf.stack([-pad_top, -pad_left, pad_bottom+height, pad_right+width], -1) / [height, width, height, width]\n","    modified_gt_boxes = renormalize_bboxes_with_min_max(gt_boxes, min_max)\n","\n","    return expanded_image, modified_gt_boxes\n","\n","def patch(img, gt_boxes):\n","    \"\"\"Generating random patch and adjusting image and ground truth objects to this patch.\n","    After this operation some of the ground truth boxes / objects could be removed from the image.\n","    However, these objects are not excluded from the output, only the coordinates are changed as zero.\n","    inputs:\n","        img = (height, width, depth)\n","        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","            in normalized form [0, 1]\n","    outputs:\n","        modified_img = (final_height, final_width, depth)\n","        modified_gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n","            in normalized form [0, 1]\n","    \"\"\"\n","    img_shape = tf.cast(tf.shape(img), dtype=tf.float32)\n","    org_height, org_width = img_shape[0], img_shape[1]\n","    # Randomly expand image and adjust bounding boxes\n","    img, gt_boxes = randomly_apply_operation(expand_image, img, gt_boxes, org_height, org_width)\n","    # Get random minimum overlap value\n","    min_overlap = get_random_min_overlap()\n","\n","    begin, size, new_boundaries = tf.image.sample_distorted_bounding_box(\n","        tf.shape(img),\n","        # use_image_if_no_bounding_boxes=True, ### FIX:26/1/24\n","        bounding_boxes=tf.expand_dims(gt_boxes, 0),\n","        aspect_ratio_range=[0.5, 2.0],\n","        min_object_covered=min_overlap)\n","\n","    img = tf.slice(img, begin, size)\n","    img = tf.image.resize(img, (org_height, org_width))\n","    gt_boxes = renormalize_bboxes_with_min_max(gt_boxes, new_boundaries[0, 0])\n","\n","    return img, gt_boxes"]},{"cell_type":"markdown","metadata":{"id":"qQE2tvn-Fhil"},"source":["# bbox_utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:03:23.305699Z","iopub.status.busy":"2024-02-14T08:03:23.305410Z","iopub.status.idle":"2024-02-14T08:03:23.327317Z","shell.execute_reply":"2024-02-14T08:03:23.326649Z","shell.execute_reply.started":"2024-02-14T08:03:23.305676Z"},"id":"knGVG_zdT4Sf","trusted":true},"outputs":[],"source":["def non_max_suppression(pred_bboxes, pred_labels, **kwargs):\n","    \"\"\"Applying non maximum suppression.\n","    SSD uses non-maximum suppression to prune away boxes that have IOU overlap with previously selected boxes.\n","    Details could be found on tensorflow documentation.\n","    https://www.tensorflow.org/api_docs/python/tf/image/combined_non_max_suppression\n","    inputs:\n","        pred_bboxes = (batch_size, total_bboxes, total_labels, [y1, x1, y2, x2])\n","            total_labels should be 1 for binary operations like in rpn\n","        pred_labels = (batch_size, total_bboxes, total_labels)\n","        **kwargs = other parameters\n","\n","    outputs:\n","        nms_boxes = (batch_size, max_detections, [y1, x1, y2, x2])\n","        nmsed_scores = (batch_size, max_detections)\n","        nmsed_classes = (batch_size, max_detections)\n","        valid_detections = (batch_size)\n","            Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid.\n","            The rest of the entries are zero paddings.\n","    \"\"\"\n","    return tf.image.combined_non_max_suppression(\n","        pred_bboxes,\n","        pred_labels,\n","        **kwargs\n","    )\n","\n","def generate_iou_map(bboxes, gt_boxes, transpose_perm=[0, 2, 1]):\n","    \"\"\"Calculating intersection over union values for each ground truth boxes in a dynamic manner.\n","    It is supported from 1d to 3d dimensions for bounding boxes.\n","    Even if bboxes have different rank from gt_boxes it should be work.\n","    inputs:\n","        bboxes = (dynamic_dimension, [y1, x1, y2, x2])\n","        gt_boxes = (dynamic_dimension, [y1, x1, y2, x2])\n","        transpose_perm = (transpose_perm_order)\n","            for 3d gt_boxes => [0, 2, 1]\n","            The returned tensor's dimension i will correspond to the input dimension perm[i].\n","\n","    outputs:\n","        iou_map = (dynamic_dimension, total_gt_boxes)\n","            same rank with the gt_boxes\n","    \"\"\"\n","    gt_rank = tf.rank(gt_boxes)\n","    gt_expand_axis = gt_rank - 2\n","\n","    bbox_y1, bbox_x1, bbox_y2, bbox_x2 = tf.split(bboxes, 4, axis=-1)\n","    gt_y1, gt_x1, gt_y2, gt_x2 = tf.split(gt_boxes, 4, axis=-1)\n","\n","    # Calculate bbox and ground truth boxes areas\n","    gt_area = tf.squeeze((gt_y2 - gt_y1) * (gt_x2 - gt_x1), axis=-1)\n","    bbox_area = tf.squeeze((bbox_y2 - bbox_y1) * (bbox_x2 - bbox_x1), axis=-1)\n","\n","    x_top = tf.maximum(bbox_x1, tf.transpose(gt_x1, transpose_perm))\n","    y_top = tf.maximum(bbox_y1, tf.transpose(gt_y1, transpose_perm))\n","    x_bottom = tf.minimum(bbox_x2, tf.transpose(gt_x2, transpose_perm))\n","    y_bottom = tf.minimum(bbox_y2, tf.transpose(gt_y2, transpose_perm))\n","\n","    # Calculate intersection area\n","    intersection_area = tf.maximum(x_bottom - x_top, 0) * tf.maximum(y_bottom - y_top, 0)\n","    # Calculate union area\n","    union_area = (tf.expand_dims(bbox_area, -1) + tf.expand_dims(gt_area, gt_expand_axis) - intersection_area)\n","    # Intersection over Union\n","    return intersection_area / union_area\n","\n","def get_bboxes_from_deltas(prior_boxes, deltas):\n","    \"\"\"Calculating bounding boxes for given bounding box and delta values.\n","    inputs:\n","        prior_boxes = (total_bboxes, [y1, x1, y2, x2])\n","        deltas = (batch_size, total_bboxes, [delta_y, delta_x, delta_h, delta_w])\n","\n","    outputs:\n","        final_boxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n","    \"\"\"\n","    all_pbox_width = prior_boxes[..., 3] - prior_boxes[..., 1]\n","    all_pbox_height = prior_boxes[..., 2] - prior_boxes[..., 0]\n","    all_pbox_ctr_x = prior_boxes[..., 1] + 0.5 * all_pbox_width\n","    all_pbox_ctr_y = prior_boxes[..., 0] + 0.5 * all_pbox_height\n","\n","    all_bbox_width = tf.exp(deltas[..., 3]) * all_pbox_width\n","    all_bbox_height = tf.exp(deltas[..., 2]) * all_pbox_height\n","    all_bbox_ctr_x = (deltas[..., 1] * all_pbox_width) + all_pbox_ctr_x\n","    all_bbox_ctr_y = (deltas[..., 0] * all_pbox_height) + all_pbox_ctr_y\n","\n","    # Calculate coordinates of predicted bounding box\n","    y1 = all_bbox_ctr_y - (0.5 * all_bbox_height)\n","    x1 = all_bbox_ctr_x - (0.5 * all_bbox_width)\n","    y2 = all_bbox_height + y1\n","    x2 = all_bbox_width + x1\n","\n","    return tf.stack([y1, x1, y2, x2], axis=-1)\n","\n","def get_deltas_from_bboxes(bboxes, gt_boxes):\n","    \"\"\"Calculating bounding box deltas for given bounding box and ground truth boxes.\n","    inputs:\n","        bboxes = (total_bboxes, [y1, x1, y2, x2])\n","        gt_boxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n","\n","    outputs:\n","        final_deltas = (batch_size, total_bboxes, [delta_y, delta_x, delta_h, delta_w])\n","    \"\"\"\n","    bbox_width = bboxes[..., 3] - bboxes[..., 1]\n","    bbox_height = bboxes[..., 2] - bboxes[..., 0]\n","    bbox_ctr_x = bboxes[..., 1] + 0.5 * bbox_width\n","    bbox_ctr_y = bboxes[..., 0] + 0.5 * bbox_height\n","\n","    try:\n","        gt_width = gt_boxes[..., 3] - gt_boxes[..., 1]\n","        gt_height = gt_boxes[..., 2] - gt_boxes[..., 0]\n","        gt_ctr_x = gt_boxes[..., 1] + 0.5 * gt_width\n","        gt_ctr_y = gt_boxes[..., 0] + 0.5 * gt_height\n","    except:\n","        tf.print(gt_boxes)\n","        tf.print(gt_boxes.shape)\n","\n","    # tf.where(condition, x, y) where values in x is replaced with y if false\n","    bbox_width = tf.where(tf.equal(bbox_width, 0), 1e-3, bbox_width)\n","    bbox_height = tf.where(tf.equal(bbox_height, 0), 1e-3, bbox_height)\n","\n","    delta_x = tf.where(tf.equal(gt_width, 0), tf.zeros_like(gt_width), tf.truediv((gt_ctr_x - bbox_ctr_x), bbox_width)) # 0 or offset/bbox_wdith\n","    delta_y = tf.where(tf.equal(gt_height, 0), tf.zeros_like(gt_height), tf.truediv((gt_ctr_y - bbox_ctr_y), bbox_height))\n","    delta_w = tf.where(tf.equal(gt_width, 0), tf.zeros_like(gt_width), tf.math.log(gt_width / bbox_width)) # 0 or ln(gt_width/bbox_width)\n","    delta_h = tf.where(tf.equal(gt_height, 0), tf.zeros_like(gt_height), tf.math.log(gt_height / bbox_height))\n","    return tf.stack([delta_y, delta_x, delta_h, delta_w], axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:03:23.328576Z","iopub.status.busy":"2024-02-14T08:03:23.328317Z","iopub.status.idle":"2024-02-14T08:03:23.341607Z","shell.execute_reply":"2024-02-14T08:03:23.340695Z","shell.execute_reply.started":"2024-02-14T08:03:23.328553Z"},"id":"FngGpkXMT8F1","trusted":true},"outputs":[],"source":["def renormalize_bboxes_with_min_max(bboxes, min_max):\n","    \"\"\"Renormalizing given bounding boxes to the new boundaries.\n","    r = (x - min) / (max - min)\n","    inputs:\n","        bboxes = (total_bboxes, [y1, x1, y2, x2])\n","        min_max = ([y_min, x_min, y_max, x_max])\n","\n","    outputs:\n","        normalized_bboxes = (total_bboxes, [y1, x1, y2, x2])\n","            in normalized form [0, 1]\n","    \"\"\"\n","    y_min, x_min, y_max, x_max = tf.split(min_max, 4)\n","    renomalized_bboxes = bboxes - tf.concat([y_min, x_min, y_min, x_min], -1)\n","    renomalized_bboxes /= tf.concat([y_max-y_min, x_max-x_min, y_max-y_min, x_max-x_min], -1)\n","    return tf.clip_by_value(renomalized_bboxes, 0, 1)\n","\n","def normalize_bboxes(bboxes, height, width):\n","    \"\"\"Normalizing bounding boxes.\n","    inputs:\n","        bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n","        height = image height\n","        width = image width\n","\n","    outputs:\n","        normalized_bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n","            in normalized form [0, 1]\n","    \"\"\"\n","    y1 = bboxes[..., 0] / height\n","    x1 = bboxes[..., 1] / width\n","    y2 = bboxes[..., 2] / height\n","    x2 = bboxes[..., 3] / width\n","    return tf.stack([y1, x1, y2, x2], axis=-1)\n","\n","def denormalize_bboxes(bboxes, height, width):\n","    \"\"\"Denormalizing bounding boxes.\n","    inputs:\n","        bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n","            in normalized form [0, 1]\n","        height = image height\n","        width = image width\n","\n","    outputs:\n","        denormalized_bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n","    \"\"\"\n","    y1 = bboxes[..., 0] * height\n","    x1 = bboxes[..., 1] * width\n","    y2 = bboxes[..., 2] * height\n","    x2 = bboxes[..., 3] * width\n","    return tf.round(tf.stack([y1, x1, y2, x2], axis=-1))"]},{"cell_type":"markdown","metadata":{"id":"hRkoKNc-F1Yw"},"source":["# drawing_utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:03:23.343241Z","iopub.status.busy":"2024-02-14T08:03:23.342949Z","iopub.status.idle":"2024-02-14T08:03:23.360143Z","shell.execute_reply":"2024-02-14T08:03:23.359471Z","shell.execute_reply.started":"2024-02-14T08:03:23.343217Z"},"id":"ambu0h84FoUV","trusted":true},"outputs":[],"source":["def draw_grid_map(img, grid_map, stride):\n","    \"\"\"Drawing grid intersection on given image.\n","    inputs:\n","        img = (height, width, channels)\n","        grid_map = (output_height * output_width, [y_index, x_index, y_index, x_index])\n","            tiled x, y coordinates\n","        stride = number of stride\n","\n","    outputs:\n","        array = (height, width, channels)\n","    \"\"\"\n","    image = Image.fromarray(img)\n","    draw = ImageDraw.Draw(image)\n","    counter = 0\n","    for grid in grid_map:\n","        draw.rectangle((\n","            grid[0] + stride // 2 - 2,\n","            grid[1] + stride // 2 - 2,\n","            grid[2] + stride // 2 + 2,\n","            grid[3] + stride // 2 + 2), fill=(255, 255, 255, 0))\n","        counter += 1\n","    plt.figure()\n","    plt.imshow(image)\n","    plt.show()\n","\n","def draw_bboxes(imgs, bboxes):\n","    \"\"\"Drawing bounding boxes on given images.\n","    inputs:\n","        imgs = (batch_size, height, width, channels)\n","        bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n","            in normalized form [0, 1]\n","    \"\"\"\n","    colors = tf.constant([[1, 0, 0, 1]], dtype=tf.float32)\n","    imgs_with_bb = tf.image.draw_bounding_boxes(imgs, bboxes, colors)\n","    plt.figure()\n","    for img_with_bb in imgs_with_bb:\n","        plt.imshow(img_with_bb)\n","        plt.show()\n","\n","def draw_bboxes_with_labels(img, bboxes, label_indices, probs, labels):\n","    \"\"\"Drawing bounding boxes with labels on given image.\n","    inputs:\n","        img = (height, width, channels)\n","        bboxes = (total_bboxes, [y1, x1, y2, x2])\n","            in denormalized form\n","        label_indices = (total_bboxes)\n","        probs = (total_bboxes)\n","        labels = [labels string list]\n","    \"\"\"\n","    colors = tf.random.uniform((len(labels), 4), maxval=256, dtype=tf.int32)\n","    image = tf.keras.preprocessing.image.array_to_img(img)\n","    width, height = image.size\n","    draw = ImageDraw.Draw(image)\n","    for index, bbox in enumerate(bboxes):\n","        y1, x1, y2, x2 = tf.split(bbox, 4)\n","        width = x2 - x1\n","        height = y2 - y1\n","        if width <= 0 or height <= 0:\n","            continue\n","        label_index = int(label_indices[index])\n","        color = tuple(colors[label_index].numpy())\n","        label_text = \"{0} {1:0.3f}\".format(labels[label_index], probs[index])\n","        draw.text((x1 + 4, y1 + 2), label_text, fill=color)\n","        draw.rectangle((x1, y1, x2, y2), outline=color, width=3)\n","    #\n","    plt.figure()\n","    plt.imshow(image)\n","    plt.show()\n","\n","def draw_predictions(dataset, pred_bboxes, pred_labels, pred_scores, labels, batch_size):\n","    for batch_id, image_data in enumerate(dataset):\n","        imgs, _, _ = image_data\n","        img_size = imgs.shape[1]\n","        start = batch_id * batch_size\n","        end = start + batch_size\n","        batch_bboxes, batch_labels, batch_scores = pred_bboxes[start:end], pred_labels[start:end], pred_scores[start:end]\n","        for i, img in enumerate(imgs):\n","            denormalized_bboxes = denormalize_bboxes(batch_bboxes[i], img_size, img_size)\n","            draw_bboxes_with_labels(img, denormalized_bboxes, batch_labels[i], batch_scores[i], labels)"]},{"cell_type":"markdown","metadata":{"id":"rZPgvuNbGMN_"},"source":["# io_utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:03:23.361665Z","iopub.status.busy":"2024-02-14T08:03:23.361266Z","iopub.status.idle":"2024-02-14T08:03:23.373249Z","shell.execute_reply":"2024-02-14T08:03:23.372579Z","shell.execute_reply.started":"2024-02-14T08:03:23.361633Z"},"id":"u9QDiqrrGLm3","trusted":true},"outputs":[],"source":["def get_log_path(model_type, custom_postfix=\"\"):\n","    \"\"\"Generating log path from model_type value for tensorboard.\n","    inputs:\n","        model_type = \"mobilenet_v2\"\n","        custom_postfix = any custom string for log folder name\n","\n","    outputs:\n","        log_path = tensorboard log path, for example: \"logs/mobilenet_v2/{date}\"\n","    \"\"\"\n","    return \"/content/drive/MyDrive/logs/{}{}/{}\".format(model_type, custom_postfix, datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","\n","def get_model_path(model_type):\n","    \"\"\"Generating model path from model_type value for save/load model weights.\n","    inputs:\n","        model_type = \"vgg16\", \"mobilenet_v2\"\n","\n","    outputs:\n","        model_path = os model path, for example: \"trained/ssd_vgg16_model_weights.h5\"\n","    \"\"\"\n","    main_path = \"/content/drive/MyDrive/trained\"\n","    if not os.path.exists(main_path):\n","        os.makedirs(main_path)\n","    model_path = os.path.join(main_path, \"ssd_{}_model_weights.h5\".format(model_type))\n","    return model_path"]},{"cell_type":"markdown","metadata":{"id":"odgPsWcyGRBW"},"source":["# train_utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:03:23.374783Z","iopub.status.busy":"2024-02-14T08:03:23.374460Z","iopub.status.idle":"2024-02-14T08:03:23.388039Z","shell.execute_reply":"2024-02-14T08:03:23.387216Z","shell.execute_reply.started":"2024-02-14T08:03:23.374752Z"},"id":"NFs1NL6aGQkU","trusted":true},"outputs":[],"source":["def scheduler(epoch):\n","    \"\"\"Generating learning rate value for a given epoch.\n","    inputs:\n","        epoch = number of current epoch\n","\n","    outputs:\n","        learning_rate = float learning rate value\n","    \"\"\"\n","    if epoch < 100:\n","        return hyper_params[\"lr\"]\n","    elif epoch < 125:\n","        return hyper_params[\"lr\"]*1e-1\n","    else:\n","        return hyper_params[\"lr\"]*1e-2\n","\n","def get_step_size(total_items, batch_size):\n","    \"\"\"Get step size for given total item size and batch size.\n","    inputs:\n","        total_items = number of total items\n","        batch_size = number of batch size during training or validation\n","\n","    outputs:\n","        step_size = number of step size for model training\n","    \"\"\"\n","    return math.ceil(total_items / batch_size)\n","\n","def calculate_actual_outputs(prior_boxes, gt_boxes, gt_labels, hyper_params):\n","    \"\"\"Calculate ssd actual output values.\n","    Batch operations supported.\n","    inputs:\n","        prior_boxes = (total_prior_boxes, [y1, x1, y2, x2])\n","            these values in normalized format between [0, 1]\n","        gt_boxes (batch_size, gt_box_size, [y1, x1, y2, x2])\n","            these values in normalized format between [0, 1]\n","        gt_labels (batch_size, gt_box_size)\n","        hyper_params = dictionary\n","\n","    outputs:\n","        bbox_deltas = (batch_size, total_bboxes, [delta_y, delta_x, delta_h, delta_w])\n","        bbox_labels = (batch_size, total_bboxes, [0,0,...,0])\n","            labels are one-hot encoded\n","    \"\"\"\n","    batch_size = tf.shape(gt_boxes)[0]\n","    total_labels = hyper_params[\"total_labels\"]\n","    iou_threshold = hyper_params[\"iou_threshold\"]\n","    variances = hyper_params[\"variances\"]\n","    # Number of default bbox\n","    total_prior_boxes = prior_boxes.shape[0]\n","    # Calculate iou values between each bboxes and ground truth boxes\n","    iou_map = generate_iou_map(prior_boxes, gt_boxes)\n","    # Get max index value for each row\n","    max_indices_each_gt_box = tf.argmax(iou_map, axis=2, output_type=tf.int32)\n","    # IoU map has iou values for every gt boxes and we merge these values column wise\n","    merged_iou_map = tf.reduce_max(iou_map, axis=2)\n","\n","    pos_cond = tf.greater(merged_iou_map, iou_threshold)\n","    gt_boxes_map = tf.gather(gt_boxes, max_indices_each_gt_box, batch_dims=1)\n","    expanded_gt_boxes = tf.where(tf.expand_dims(pos_cond, -1), gt_boxes_map, tf.zeros_like(gt_boxes_map))\n","    bbox_deltas = get_deltas_from_bboxes(prior_boxes, expanded_gt_boxes) / variances\n","\n","    gt_labels_map = tf.gather(gt_labels, max_indices_each_gt_box, batch_dims=1)\n","    expanded_gt_labels = tf.where(pos_cond, gt_labels_map, tf.zeros_like(gt_labels_map))\n","    bbox_labels = tf.one_hot(expanded_gt_labels, total_labels)\n","    return bbox_deltas, bbox_labels\n","\n","\n","def generator(dataset, prior_boxes, hyper_params):\n","    \"\"\"Tensorflow data generator for fit method, yielding inputs and outputs.\n","    inputs:\n","        dataset = tf.data.Dataset, PaddedBatchDataset\n","        prior_boxes = (total_prior_boxes, [y1, x1, y2, x2])\n","            these values in normalized format between [0, 1]\n","        hyper_params = dictionary\n","\n","    outputs:\n","        yield inputs, outputs\n","    \"\"\"\n","    while True:\n","        try:\n","            for image_data in dataset:\n","                img, gt_boxes, gt_labels = image_data\n","                # Calculate outputs for training\n","                actual_deltas, actual_labels = calculate_actual_outputs(prior_boxes, gt_boxes, gt_labels, hyper_params)\n","                yield img, (actual_deltas, actual_labels)\n","        except StopIteration:\n","            pass\n","        except:\n","            tf.print(img.shape, gt_boxes.shape, gt_labels.shape)\n","            tf.print(gt_boxes)\n","            pass"]},{"cell_type":"markdown","metadata":{"id":"KZ9rhI9ZGEKQ"},"source":["# eval_utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:03:23.391416Z","iopub.status.busy":"2024-02-14T08:03:23.390869Z","iopub.status.idle":"2024-02-14T08:03:23.411904Z","shell.execute_reply":"2024-02-14T08:03:23.411207Z","shell.execute_reply.started":"2024-02-14T08:03:23.391390Z"},"id":"GIG-uml-F27J","trusted":true},"outputs":[],"source":["def init_stats(labels):\n","    \"\"\"Initialise statistics used in evaluation.\n","    inputs:\n","        labels (list)\n","\n","    outputs:\n","        stats (dict)\n","    \"\"\"\n","    stats = {}\n","    for i, label in enumerate(labels):\n","        if i == 0: # first element is bg\n","            continue\n","        stats[i] = {\n","            \"label\": label,\n","            \"total\": 0,\n","            \"tp\": [],\n","            \"fp\": [],\n","            \"scores\": [],\n","        }\n","    return stats\n","\n","def update_stats(pred_bboxes, pred_labels, pred_scores, gt_boxes, gt_labels, stats):\n","    # Calculate iou values between predicted bboxes and ground truth boxes\n","    iou_map = generate_iou_map(pred_bboxes, gt_boxes)\n","    merged_iou_map = tf.reduce_max(iou_map, axis=-1)\n","    max_indices_each_gt = tf.argmax(iou_map, axis=-1, output_type=tf.int32)\n","    sorted_ids = tf.argsort(merged_iou_map, direction=\"DESCENDING\")\n","\n","    count_holder = tf.unique_with_counts(tf.reshape(gt_labels, (-1,)))\n","    for i, gt_label in enumerate(count_holder[0]):\n","        if gt_label == -1:\n","            continue\n","        gt_label = int(gt_label)\n","        stats[gt_label][\"total\"] += int(count_holder[2][i])\n","    for batch_id, m in enumerate(merged_iou_map):\n","        true_labels = []\n","        for i, sorted_id in enumerate(sorted_ids[batch_id]):\n","            pred_label = pred_labels[batch_id, sorted_id]\n","            if pred_label == 0:\n","                continue\n","\n","            iou = merged_iou_map[batch_id, sorted_id]\n","            gt_id = max_indices_each_gt[batch_id, sorted_id]\n","            gt_label = int(gt_labels[batch_id, gt_id])\n","            pred_label = int(pred_label)\n","            score = pred_scores[batch_id, sorted_id]\n","            stats[pred_label][\"scores\"].append(score)\n","            stats[pred_label][\"tp\"].append(0)\n","            stats[pred_label][\"fp\"].append(0)\n","            if iou >= 0.5 and pred_label == gt_label and gt_id not in true_labels:\n","                stats[pred_label][\"tp\"][-1] = 1\n","                true_labels.append(gt_id)\n","            else:\n","                stats[pred_label][\"fp\"][-1] = 1\n","    return stats\n","\n","def calculate_ap(recall, precision):\n","    \"\"\"Calculate Average Precision (AP).\n","    \"\"\"\n","    ap = 0\n","    for r in np.arange(0, 1.1, 0.1):\n","        prec_rec = precision[recall >= r]\n","        if len(prec_rec) > 0:\n","            ap += np.amax(prec_rec)\n","    # By definition AP = sum(max(precision whose recall is above r))/11\n","    ap /= 11\n","    return ap\n","\n","def calculate_mAP(stats):\n","    aps = []\n","    for label in stats:\n","        label_stats = stats[label]\n","        tp = np.array(label_stats[\"tp\"])\n","        fp = np.array(label_stats[\"fp\"])\n","        scores = np.array(label_stats[\"scores\"])\n","        ids = np.argsort(-scores)\n","        total = label_stats[\"total\"]\n","        accumulated_tp = np.cumsum(tp[ids])\n","        accumulated_fp = np.cumsum(fp[ids])\n","        recall = accumulated_tp / total\n","        precision = accumulated_tp / (accumulated_fp + accumulated_tp)\n","        ap = calculate_ap(recall, precision)\n","        stats[label][\"recall\"] = recall\n","        stats[label][\"precision\"] = precision\n","        stats[label][\"AP\"] = ap\n","        aps.append(ap)\n","    mAP = np.mean(aps)\n","    return stats, mAP\n","\n","def evaluate_predictions(dataset, pred_bboxes, pred_labels, pred_scores, labels, batch_size):\n","    stats = init_stats(labels)\n","    for batch_id, image_data in enumerate(dataset):\n","        imgs, gt_boxes, gt_labels = image_data\n","        # try:\n","        #     imgs, gt_boxes, gt_labels = image_data\n","        # except:\n","        #     imgs, gt_boxes, gt_labels = image_data[0], image_data[1][0], image_data[1][1]\n","        start = batch_id * batch_size\n","        end = start + batch_size\n","        batch_bboxes, batch_labels, batch_scores = pred_bboxes[start:end], pred_labels[start:end], pred_scores[start:end]\n","        stats = update_stats(batch_bboxes, batch_labels, batch_scores, gt_boxes, gt_labels, stats)\n","    stats, mAP = calculate_mAP(stats)\n","    print(\"mAP: {}\".format(float(mAP)))\n","    return stats, mAP"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:03:23.413313Z","iopub.status.busy":"2024-02-14T08:03:23.413057Z","iopub.status.idle":"2024-02-14T08:03:23.426969Z","shell.execute_reply":"2024-02-14T08:03:23.426236Z","shell.execute_reply.started":"2024-02-14T08:03:23.413290Z"},"id":"T2QH8nntIUUq","trusted":true},"outputs":[],"source":["class MeanAveragePrecisionCallback(tf.keras.callbacks.Callback):\n","    \"\"\"Calculate Mean Average Precision (mAP) at the end of every epoch.\n","    Early stop and saves model with best mAP.\n","    \"\"\"\n","    def __init__(self, val_data, val_steps, labels, prior_boxes, hyper_params, batch_size, patience, model_save_path, **kwargs):\n","        super(MeanAveragePrecisionCallback, self).__init__(**kwargs)\n","        self.val_data = val_data\n","        self.val_steps = val_steps\n","        self.labels = labels\n","        self.prior_boxes = prior_boxes\n","        self.hyper_params = hyper_params\n","        self.batch_size = batch_size\n","        self.best_mAP = 0.0\n","        self.mAP_values = []  # To store mAP values at each epoch\n","        self.patience = patience\n","        self.wait = 0\n","        self.model_save_path = model_save_path\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        ssd_decoder_model = get_decoder_model(self.model, self.prior_boxes, self.hyper_params)\n","        pred_bboxes, pred_labels, pred_scores = ssd_decoder_model.predict(self.val_data, steps=self.val_steps, verbose=1)\n","        stats, mAP = evaluate_predictions(self.val_data, pred_bboxes, pred_labels, pred_scores, self.labels, self.batch_size)\n","        self.mAP_values.append(mAP)\n","\n","        if mAP > self.best_mAP:\n","            self.wait = 0\n","            self.best_mAP = mAP\n","            # self.model.save_weights(self.model_save_path)\n","        else:\n","            self.wait += 1\n","\n","        if self.wait >= self.patience:\n","            print(f\"Early stopping at epoch {epoch + 1} due to lack of improvement.\")\n","            self.model.stop_training = True\n","\n","    def on_train_begin(self, epoch, logs=None):\n","        self.best_mAP = 0.0\n","        self.wait = 0\n","        self.mAP_values = []\n","\n","    def on_train_end(self, logs=None):\n","        print(f'\\nBest Mean Average Precision: {self.best_mAP}\\n')\n","\n","    def get_mAP_values(self):\n","        return self.mAP_values"]},{"cell_type":"markdown","metadata":{"id":"BnvrtbMJGcah"},"source":["# decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:03:23.428304Z","iopub.status.busy":"2024-02-14T08:03:23.428014Z","iopub.status.idle":"2024-02-14T08:03:23.442472Z","shell.execute_reply":"2024-02-14T08:03:23.441481Z","shell.execute_reply.started":"2024-02-14T08:03:23.428280Z"},"id":"eLt0sLKBGYj-","trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import Layer, Input, Conv2D, MaxPool2D\n","from tensorflow.keras.models import Model\n","\n","class SSDDecoder(Layer):\n","    \"\"\"Generating bounding boxes and labels from ssd predictions.\n","    First calculating the boxes from predicted deltas and label probs.\n","    Then applied non max suppression and selecting top_n boxes by scores.\n","    inputs:\n","        pred_deltas = (batch_size, total_prior_boxes, [delta_y, delta_x, delta_h, delta_w])\n","        pred_label_probs = (batch_size, total_prior_boxes, [0,0,...,0])\n","    outputs:\n","        pred_bboxes = (batch_size, top_n, [y1, x1, y2, x2])\n","        pred_labels = (batch_size, top_n)\n","            1 to total label number\n","        pred_scores = (batch_size, top_n)\n","    \"\"\"\n","    def __init__(self, prior_boxes, variances, max_total_size=200, score_threshold=0.5, **kwargs):\n","        super(SSDDecoder, self).__init__(**kwargs)\n","        self.prior_boxes = prior_boxes\n","        self.variances = variances\n","        self.max_total_size = max_total_size\n","        self.score_threshold = score_threshold\n","\n","    def get_config(self):\n","        config = super(SSDDecoder, self).get_config()\n","        config.update({\n","            \"prior_boxes\": self.prior_boxes.numpy(),\n","            \"variances\": self.variances,\n","            \"max_total_size\": self.max_total_size,\n","            \"score_threshold\": self.score_threshold\n","        })\n","        return config\n","\n","    def call(self, inputs):\n","        pred_deltas = inputs[0]\n","        pred_label_probs = inputs[1]\n","        batch_size = tf.shape(pred_deltas)[0]\n","\n","        pred_deltas *= self.variances\n","        pred_bboxes = get_bboxes_from_deltas(self.prior_boxes, pred_deltas)\n","\n","        pred_labels_map = tf.expand_dims(tf.argmax(pred_label_probs, -1), -1)\n","        pred_labels = tf.where(tf.not_equal(pred_labels_map, 0), pred_label_probs, tf.zeros_like(pred_label_probs))\n","        # Reshape bboxes for non max suppression\n","        pred_bboxes = tf.reshape(pred_bboxes, (batch_size, -1, 1, 4))\n","\n","        final_bboxes, final_scores, final_labels, _ = non_max_suppression(\n","                                                                    pred_bboxes, pred_labels,\n","                                                                    max_output_size_per_class=self.max_total_size,\n","                                                                    max_total_size=self.max_total_size,\n","                                                                    score_threshold=self.score_threshold)\n","        return final_bboxes, final_labels, final_scores\n","\n","def get_decoder_model(base_model, prior_boxes, hyper_params):\n","    \"\"\"Decoding ssd predictions to valid bounding boxes and labels.\n","    inputs:\n","        base_model = tf.keras.model, base ssd model\n","        prior_boxes = (total_prior_boxes, [y1, x1, y2, x2])\n","            these values in normalized format between [0, 1]\n","        hyper_params = dictionary\n","\n","    outputs:\n","        ssd_decoder_model = tf.keras.model\n","    \"\"\"\n","    bboxes, classes, scores = SSDDecoder(prior_boxes, hyper_params[\"variances\"])(base_model.output)\n","    return Model(inputs=base_model.input, outputs=[bboxes, classes, scores])"]},{"cell_type":"markdown","metadata":{"id":"mEFNZpWUGoyx"},"source":["# header"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:03:23.443819Z","iopub.status.busy":"2024-02-14T08:03:23.443534Z","iopub.status.idle":"2024-02-14T08:03:23.457350Z","shell.execute_reply":"2024-02-14T08:03:23.456482Z","shell.execute_reply.started":"2024-02-14T08:03:23.443796Z"},"id":"c0N10FKZGnBk","trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import Layer, Input, Conv2D, MaxPool2D, Activation, SeparableConv2D\n","\n","class HeadWrapper(Layer):\n","    \"\"\"Merging all feature maps for detections.\n","    inputs:\n","        conv4_3 = (batch_size, (layer_shape x aspect_ratios), last_dimension)\n","            ssd300 conv4_3 shape => (38 x 38 x 4) = 5776\n","        conv7 = (batch_size, (layer_shape x aspect_ratios), last_dimension)\n","            ssd300 conv7 shape => (19 x 19 x 6) = 2166\n","        conv8_2 = (batch_size, (layer_shape x aspect_ratios), last_dimension)\n","            ssd300 conv8_2 shape => (10 x 10 x 6) = 600\n","        conv9_2 = (batch_size, (layer_shape x aspect_ratios), last_dimension)\n","            ssd300 conv9_2 shape => (5 x 5 x 6) = 150\n","        conv10_2 = (batch_size, (layer_shape x aspect_ratios), last_dimension)\n","            ssd300 conv10_2 shape => (3 x 3 x 4) = 36\n","        conv11_2 = (batch_size, (layer_shape x aspect_ratios), last_dimension)\n","            ssd300 conv11_2 shape => (1 x 1 x 4) = 4\n","                                           Total = 8732 default box\n","\n","    outputs:\n","        merged_head = (batch_size, total_prior_boxes, last_dimension)\n","    \"\"\"\n","\n","    def __init__(self, last_dimension, **kwargs):\n","        super(HeadWrapper, self).__init__(**kwargs)\n","        self.last_dimension = last_dimension\n","\n","    def get_config(self):\n","        config = super(HeadWrapper, self).get_config()\n","        config.update({\"last_dimension\": self.last_dimension})\n","        return config\n","\n","    def call(self, inputs):\n","        last_dimension = self.last_dimension\n","        batch_size = tf.shape(inputs[0])[0]\n","        outputs = []\n","        for conv_layer in inputs:\n","            outputs.append(tf.reshape(conv_layer, (batch_size, -1, last_dimension)))\n","        return tf.concat(outputs, axis=1)\n","\n","def get_head_from_outputs(hyper_params, outputs):\n","    \"\"\"Generating ssd bbox delta and label heads.\n","    inputs:\n","        hyper_params = dictionary\n","        outputs = list of ssd layers output to be used for prediction\n","\n","    outputs:\n","        pred_deltas = merged outputs for bbox delta head\n","        pred_labels = merged outputs for bbox label head\n","    \"\"\"\n","    total_labels = hyper_params[\"total_labels\"]\n","    # +1 for ratio 1\n","    len_aspect_ratios = [len(x) + 1 for x in hyper_params[\"aspect_ratios\"]]\n","    # print(len_aspect_ratios)\n","    labels_head = []\n","    boxes_head = []\n","    for i, output in enumerate(outputs):\n","        # print(i)\n","        aspect_ratio = len_aspect_ratios[i]\n","        # labels_head.append(Conv2D(aspect_ratio * total_labels, (3, 3), padding=\"same\", name=\"{}_conv_label_output\".format(i+1))(output))\n","        # boxes_head.append(Conv2D(aspect_ratio * 4, (3, 3), padding=\"same\", name=\"{}_conv_boxes_output\".format(i+1))(output))\n","        labels_head.append(SeparableConv2D(aspect_ratio * total_labels, (3, 3), padding=\"same\", name=\"{}_conv_label_output\".format(i+1))(output))\n","        boxes_head.append(SeparableConv2D(aspect_ratio * 4, (3, 3), padding=\"same\", name=\"{}_conv_boxes_output\".format(i+1))(output))\n","\n","    pred_labels = HeadWrapper(total_labels, name=\"labels_head\")(labels_head)\n","    pred_labels = Activation(\"softmax\", name=\"conf\")(pred_labels)\n","\n","    pred_deltas = HeadWrapper(4, name=\"loc\")(boxes_head)\n","    return pred_deltas, pred_labels"]},{"cell_type":"markdown","metadata":{"id":"gJANTXL-HBrD"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"FaalWXrEH4PN"},"source":["# ssd_loss: cross-entropy/focal and huber loss\n","\n","Focal loss for classification task and Huber loss for regression task"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:03:23.458982Z","iopub.status.busy":"2024-02-14T08:03:23.458725Z","iopub.status.idle":"2024-02-14T08:03:23.476705Z","shell.execute_reply":"2024-02-14T08:03:23.475868Z","shell.execute_reply.started":"2024-02-14T08:03:23.458959Z"},"id":"no7DmauDH51C","trusted":true},"outputs":[],"source":["class CustomLoss(object):\n","    \"\"\"Composition of Focal and Huber losses.\"\"\"\n","\n","    def __init__(self, neg_pos_ratio, loc_loss_alpha,\n","                 alpha,\n","                 gamma,\n","                 use_focal=False):\n","        \"\"\"\n","        Args:\n","            neg_pos_ratio: a float number representing the negative to positive ratio.\n","            loc_loss_alpha: a float number representing the localization loss.\n","            alpha, gamma: a float number for Focal loss formula.\n","        \"\"\"\n","        self.neg_pos_ratio = tf.constant(neg_pos_ratio, dtype=tf.float32)\n","        self.loc_loss_alpha = tf.constant(loc_loss_alpha, dtype=tf.float32)\n","        self.alpha = tf.constant(alpha, dtype=tf.float32)\n","        self.gamma = tf.constant(gamma, dtype=tf.float32)\n","        self.use_focal = use_focal\n","\n","    def loc_loss_fn(self, actual_deltas, pred_deltas):\n","        \"\"\"Calculating SSD localization loss value for only positive samples.\n","        inputs:\n","            actual_deltas = (batch_size, total_prior_boxes, [delta_y, delta_x, delta_h, delta_w])\n","            pred_deltas = (batch_size, total_prior_boxes, [delta_y, delta_x, delta_h, delta_w])\n","\n","        outputs:\n","            loc_loss = localization / bbox / regression loss value\n","        \"\"\"\n","        # Localization / bbox / regression loss calculation for all bboxes\n","        loc_loss_fn = tf.losses.Huber(reduction=tf.losses.Reduction.NONE) # delta=1.0\n","        loc_loss_for_all = loc_loss_fn(actual_deltas, pred_deltas)\n","\n","        # After tf 2.2.0 version, the huber calculates mean over the last axis\n","        loc_loss_for_all = tf.cond(tf.greater(tf.rank(loc_loss_for_all), tf.constant(2)),\n","                                   lambda: tf.reduce_sum(loc_loss_for_all, axis=-1),\n","                                   lambda: loc_loss_for_all * tf.cast(tf.shape(pred_deltas)[-1], dtype=tf.float32))\n","\n","        # Creating Positive Mask\n","        pos_cond = tf.reduce_any(tf.not_equal(actual_deltas, tf.constant(0.0)), axis=2)\n","        pos_mask = tf.cast(pos_cond, dtype=tf.float32)\n","        # Counting Total Positive Bounding Boxes\n","        total_pos_bboxes = tf.reduce_sum(pos_mask, axis=1)\n","\n","        # Calculating Localization Loss for Positive Bounding Boxes\n","        loc_loss = tf.reduce_sum(pos_mask * loc_loss_for_all, axis=-1)\n","        # Handling Cases with No Positive Bounding Boxes\n","        total_pos_bboxes = tf.where(tf.equal(total_pos_bboxes, tf.constant(0.0)), tf.constant(1.0), total_pos_bboxes)\n","        # Final Localization Loss Calculation\n","        loc_loss = loc_loss / total_pos_bboxes\n","        return loc_loss * self.loc_loss_alpha\n","\n","    def conf_loss_fn(self, actual_labels, pred_labels):\n","        \"\"\"Calculating SSD confidence loss value with hard negative mining as mentioned in the paper.\n","        Replaced CategoricalCrossentropy with CategoricalFocalCrossentropy.\n","        inputs:\n","            actual_labels = (batch_size, total_prior_boxes, total_labels)\n","            pred_labels = (batch_size, total_prior_boxes, total_labels)\n","\n","        outputs:\n","            conf_loss = confidence / class / label loss value\n","        \"\"\"\n","        # tf.print(actual_labels) # one-hot\n","        # tf.print(pred_labels) # float32\n","\n","        # Confidence / Label loss calculation for all labels\n","        if self.use_focal:\n","            conf_loss_fn = tf.keras.losses.CategoricalFocalCrossentropy(alpha=self.alpha, gamma=self.gamma, reduction=tf.losses.Reduction.NONE)\n","            conf_loss_for_all = conf_loss_fn(actual_labels, pred_labels)\n","            # tf.print(\"Confidence Loss:\", conf_loss_for_all)\n","        else:\n","            conf_loss_fn = tf.losses.CategoricalCrossentropy(reduction=tf.losses.Reduction.NONE)\n","            conf_loss_for_all = conf_loss_fn(actual_labels, pred_labels)\n","            # tf.print(\"Confidence Loss:\", conf_loss_for_all)\n","\n","        # Creating Positive Mask for non-background class\n","        pos_cond = tf.reduce_any(tf.not_equal(actual_labels[..., 1:], tf.constant(0.0)), axis=2)\n","        pos_mask = tf.cast(pos_cond, dtype=tf.float32)\n","        # Counting Total Positive Bounding Boxes\n","        total_pos_bboxes = tf.reduce_sum(pos_mask, axis=1)\n","\n","        # Hard negative mining\n","        total_neg_bboxes = tf.cast(total_pos_bboxes * self.neg_pos_ratio, tf.int32)\n","\n","        masked_loss = conf_loss_for_all * actual_labels[..., 0]\n","        sorted_loss = tf.argsort(masked_loss, direction=\"DESCENDING\")\n","        sorted_loss = tf.argsort(sorted_loss)\n","        neg_cond = tf.less(sorted_loss, tf.expand_dims(total_neg_bboxes, axis=1))\n","        neg_mask = tf.cast(neg_cond, dtype=tf.float32)\n","\n","        final_mask = pos_mask + neg_mask\n","        conf_loss = tf.reduce_sum(final_mask * conf_loss_for_all, axis=-1)\n","        total_pos_bboxes = tf.where(tf.equal(total_pos_bboxes, tf.constant(0.0)), tf.constant(1.0), total_pos_bboxes)\n","        conf_loss = conf_loss / total_pos_bboxes\n","\n","        return conf_loss"]},{"cell_type":"markdown","metadata":{"id":"qpEJH16WFrWQ"},"source":["# data_utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:03:23.478288Z","iopub.status.busy":"2024-02-14T08:03:23.478025Z","iopub.status.idle":"2024-02-14T08:03:23.504076Z","shell.execute_reply":"2024-02-14T08:03:23.503266Z","shell.execute_reply.started":"2024-02-14T08:03:23.478265Z"},"id":"1RSbUDlZFp-4","trusted":true},"outputs":[],"source":["def preprocessing(image_data, final_height, final_width, augmentation_fn=None, evaluate=False):\n","    \"\"\"Image resizing operation handled before batch operations.\n","    inputs:\n","        image_data = tensorflow dataset image_data\n","        final_height = final image height after resizing\n","        final_width = final image width after resizing\n","\n","    outputs:\n","        img = (final_height, final_width, channels)\n","        gt_boxes = (gt_box_size, [y1, x1, y2, x2])\n","        gt_labels = (gt_box_size)\n","    \"\"\"\n","    img = image_data[\"image\"]\n","    gt_boxes = image_data[\"objects\"][\"bbox\"]\n","    gt_labels = tf.cast(image_data[\"objects\"][\"label\"] + 1, tf.int32)\n","    img = tf.image.convert_image_dtype(img, tf.float32)\n","    img = tf.image.resize(img, (final_height, final_width))\n","    if evaluate:\n","        not_diff = tf.logical_not(image_data[\"objects\"][\"is_difficult\"])\n","        gt_boxes = gt_boxes[not_diff]\n","        gt_labels = gt_labels[not_diff]\n","    if augmentation_fn:\n","        img, gt_boxes = augmentation_fn(img, gt_boxes)\n","    return img, gt_boxes, gt_labels\n","\n","def tfr_preprocessing(image_data, final_height, final_width, augmentation_fn=None, evaluate=False):\n","\n","    img = image_data[\"image/encoded\"]\n","    img = tf.io.decode_image(img, channels=3)\n","    xmin = tf.sparse.to_dense(image_data['image/object/bbox/xmin'])\n","    ymin = tf.sparse.to_dense(image_data['image/object/bbox/ymin'])\n","    xmax = tf.sparse.to_dense(image_data['image/object/bbox/xmax'])\n","    ymax = tf.sparse.to_dense(image_data['image/object/bbox/ymax'])\n","    gt_boxes = tf.stack(\n","        [ymin, xmin, ymax, xmax], axis=-1\n","    )\n","    gt_labels = tf.sparse.to_dense(image_data['image/object/class/label'])\n","    # gt_labels +=1 # VOC has idx 0 but tfr does not\n","    gt_labels = tf.cast(gt_labels, tf.int32)\n","    img = tf.image.convert_image_dtype(img, tf.float32)\n","    img.set_shape([None,None,3])\n","    img = tf.image.resize(img, (final_height, final_width))\n","    if augmentation_fn:\n","        img, gt_boxes = augmentation_fn(img, gt_boxes)\n","\n","    return img, gt_boxes, gt_labels\n","\n","def tfr_dataset(data_dir):\n","\n","    image_feature_description={\n","      'image/encoded':tf.io.FixedLenFeature([],tf.string),\n","      'image/object/bbox/xmin':tf.io.VarLenFeature(tf.float32),\n","      'image/object/bbox/ymin':tf.io.VarLenFeature(tf.float32),\n","      'image/object/bbox/xmax':tf.io.VarLenFeature(tf.float32),\n","      'image/object/bbox/ymax':tf.io.VarLenFeature(tf.float32),\n","      'image/object/class/label':tf.io.VarLenFeature(tf.int64),\n","    }\n","\n","    dataset = tf.data.TFRecordDataset(filenames=data_dir)\n","    count = 0\n","    for record in dataset:\n","        count += 1\n","    parsed_dataset = dataset.map(lambda x: tf.io.parse_single_example(x, image_feature_description))\n","\n","    return parsed_dataset, count\n","\n","def get_dataset(name, split, data_dir=\"~/tensorflow_datasets\"):\n","    \"\"\"Get tensorflow dataset split and info.\n","    inputs:\n","        name = name of the dataset, voc/2007, voc/2012, etc.\n","        split = data split string, should be one of [\"train\", \"validation\", \"test\"]\n","        data_dir = read/write path for tensorflow datasets\n","\n","    outputs:\n","        dataset = tensorflow dataset split\n","        info = tensorflow dataset info\n","    \"\"\"\n","    assert split in [\"train\", \"train+validation\", \"validation\", \"test\"]\n","    dataset, info = tfds.load(name, split=split, data_dir=data_dir, with_info=True)\n","    return dataset, info\n","\n","def get_total_item_size(info, split):\n","    \"\"\"Get total item size for given split.\n","    inputs:\n","        info = tensorflow dataset info\n","        split = data split string, should be one of [\"train\", \"validation\", \"test\"]\n","\n","    outputs:\n","        total_item_size = number of total items\n","    \"\"\"\n","    assert split in [\"train\", \"train+validation\", \"validation\", \"test\"]\n","    if split == \"train+validation\":\n","        return info.splits[\"train\"].num_examples + info.splits[\"validation\"].num_examples\n","    return info.splits[split].num_examples\n","\n","def tfr_labels(pbtxt_fname):\n","\n","    from object_detection.utils import label_map_util\n","    label_map = label_map_util.load_labelmap(pbtxt_fname)\n","    categories = label_map_util.convert_label_map_to_categories(\n","        label_map, max_num_classes=90, use_display_name=True)\n","    category_index = label_map_util.create_category_index(categories)\n","    print(categories)\n","    labels = [item['name'] for item in categories]\n","    return len(category_index.keys()), labels\n","\n","def get_labels(info):\n","    \"\"\"Get label names list.\n","    inputs:\n","        info = tensorflow dataset info\n","\n","    outputs:\n","        labels = [labels list]\n","    \"\"\"\n","    return info.features[\"labels\"].names\n","\n","def get_custom_imgs(custom_image_path):\n","    \"\"\"Generating a list of images for given path.\n","    inputs:\n","        custom_image_path = folder of the custom images\n","    outputs:\n","        custom image list = [path1, path2]\n","    \"\"\"\n","    img_paths = []\n","    for path, dir, filenames in os.walk(custom_image_path):\n","        for filename in filenames:\n","            img_paths.append(os.path.join(path, filename))\n","        break\n","    return img_paths\n","\n","def custom_data_generator(img_paths, final_height, final_width):\n","    \"\"\"Yielding custom entities as dataset.\n","    inputs:\n","        img_paths = custom image paths\n","        final_height = final image height after resizing\n","        final_width = final image width after resizing\n","    outputs:\n","        img = (final_height, final_width, depth)\n","        dummy_gt_boxes = (None, None)\n","        dummy_gt_labels = (None, )\n","    \"\"\"\n","    for img_path in img_paths:\n","        image = Image.open(img_path)\n","        resized_image = image.resize((final_width, final_height), Image.LANCZOS)\n","        img = np.array(resized_image)\n","        img = tf.image.convert_image_dtype(img, tf.float32)\n","        yield img, tf.constant([[]], dtype=tf.float32), tf.constant([], dtype=tf.int32)\n","\n","def get_data_types():\n","    \"\"\"Generating data types for tensorflow datasets.\n","    outputs:\n","        data types = output data types for (images, ground truth boxes, ground truth labels)\n","    \"\"\"\n","    return (tf.float32, tf.float32, tf.int32)\n","\n","def get_data_shapes():\n","    \"\"\"Generating data shapes for tensorflow datasets.\n","    outputs:\n","        data shapes = output data shapes for (images, ground truth boxes, ground truth labels)\n","    \"\"\"\n","    return ([None, None, None], [None, None], [None,])\n","\n","def get_padding_values():\n","    \"\"\"Generating padding values for missing values in batch for tensorflow datasets.\n","    outputs:\n","        padding values = padding values with dtypes for (images, ground truth boxes, ground truth labels)\n","    \"\"\"\n","    return (tf.constant(0, tf.float32), tf.constant(0, tf.float32), tf.constant(-1, tf.int32))"]},{"cell_type":"markdown","metadata":{"id":"amJ3m8RBjPU1"},"source":["# process Roboflow TFRecord\n","\n","\n","As of now, this only accepts TFR directly from Roboflow with apply removed from preprocessing.\n","\n","Issue: use_image_if_no_bounding_boxes=True leads to missing prediction."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T08:03:23.505881Z","iopub.status.busy":"2024-02-14T08:03:23.505193Z","iopub.status.idle":"2024-02-14T08:03:23.517519Z","shell.execute_reply":"2024-02-14T08:03:23.516797Z","shell.execute_reply.started":"2024-02-14T08:03:23.505855Z"},"id":"Djg8OEQWCOxp","trusted":true},"outputs":[],"source":["def check_for_nans(parsed_dataset, final_height, final_width):\n","    for record in parsed_dataset:\n","        img, gt_boxes, gt_labels = tfr_preprocessing(record, final_height, final_width)\n","        gt_labels = tf.cast(gt_labels, tf.float32)\n","\n","        # Check for NaN values in the image tensor\n","        img_has_nan = tf.reduce_any(tf.math.is_nan(img))\n","\n","        # Check for NaN values in the ground truth boxes tensor\n","        gt_boxes_has_nan = tf.reduce_any(tf.math.is_nan(gt_boxes))\n","\n","        # Check for NaN values in the ground truth labels tensor\n","        gt_labels_has_nan = tf.reduce_any(tf.math.is_nan(gt_labels))\n","\n","        # Print information if NaN values are found\n","        if img_has_nan or gt_boxes_has_nan or gt_labels_has_nan:\n","            print(\"NaN values found in the dataset!\")\n","            print(\"Image has NaN values:\", img_has_nan.numpy())\n","            print(\"Ground truth boxes have NaN values:\", gt_boxes_has_nan.numpy())\n","            print(\"Ground truth labels have NaN values:\", gt_labels_has_nan.numpy())\n","\n","            # You can return or handle the information accordingly\n","            return True\n","\n","    print(\"No NaN values found in the dataset.\")\n","    return False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-14T08:03:23.519394Z","iopub.status.busy":"2024-02-14T08:03:23.518740Z","iopub.status.idle":"2024-02-14T08:03:24.533988Z","shell.execute_reply":"2024-02-14T08:03:24.533105Z","shell.execute_reply.started":"2024-02-14T08:03:23.519363Z"},"executionInfo":{"elapsed":1061,"status":"ok","timestamp":1708752830789,"user":{"displayName":"happy duck","userId":"12590459388487510630"},"user_tz":-480},"id":"3uobVEH9Lidp","outputId":"086dbe8b-55d8-46a6-e5fc-028a4be95a87","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'id': 1, 'name': 'Hole'}, {'id': 2, 'name': 'Knot'}, {'id': 3, 'name': 'Line'}, {'id': 4, 'name': 'Stain'}]\n","5724 225\n","4 ['bg', 'Hole', 'Knot', 'Line', 'Stain']\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-39-1d918a581818>:56: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.ignore_errors` instead.\n"]},{"name":"stdout","output_type":"stream","text":["([None, None, None], [None, None], [None]) (<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: shape=(), dtype=int32, numpy=-1>)\n"]}],"source":["# Train Parameters\n","batch_size = hyper_params[\"batch_size\"]\n","epochs = hyper_params[\"epochs\"]\n","load_weights = False\n","img_size = hyper_params[\"img_size\"] # determined by the backbone\n","\n","if hyper_params[\"dataset\"] == 0:\n","    # dut\n","    train_files='/content/Fabric-Defect-Capstone-1/train/defects.tfrecord'\n","    val_files='/content/Fabric-Defect-Capstone-1/valid/defects.tfrecord'\n","    test_files='/content/Fabric-Defect-Capstone-1/test/defects.tfrecord'\n","    num_classes, labels = tfr_labels('/content/Fabric-Defect-Capstone-1/train/defects_label_map.pbtxt')\n","elif hyper_params[\"dataset\"] == 1:\n","    # tilda\n","    train_files='/content/TILDA-Fabric-2/train/Fabric.tfrecord'\n","    val_files='/content/TILDA-Fabric-2/valid/Fabric.tfrecord'\n","    test_files='/content/TILDA-Fabric-2/test/Fabric.tfrecord'\n","    num_classes, labels = tfr_labels('/content/TILDA-Fabric-2/train/Fabric_label_map.pbtxt')\n","elif hyper_params[\"dataset\"] == 2:\n","    # daffodil\n","    train_files='/content/Fabric-Defect-Daffodil-1/train/defects.tfrecord'\n","    val_files='/content/Fabric-Defect-Daffodil-1/valid/defects.tfrecord'\n","    test_files='/content/Fabric-Defect-Daffodil-1/test/defects.tfrecord'\n","    num_classes, labels = tfr_labels('/content/Fabric-Defect-Daffodil-1/train/defects_label_map.pbtxt')\n","elif hyper_params[\"dataset\"] == 3:\n","    # thesis\n","    train_files='/content/Fabric-Defect-Thesis-1/train/defects.tfrecord'\n","    val_files='/content/Fabric-Defect-Thesis-1/valid/defects.tfrecord'\n","    test_files='/content/Fabric-Defect-Thesis-1/test/defects.tfrecord'\n","    num_classes, labels = tfr_labels('/content/Fabric-Defect-Thesis-1/train/defects_label_map.pbtxt')\n","elif hyper_params[\"dataset\"] == 4:\n","    pass\n","\n","train_data, train_total_items = tfr_dataset(train_files)\n","val_data, val_total_items = tfr_dataset(val_files)\n","\n","# check_for_nans(train_data, img_size, img_size)\n","# check_for_nans(val_data, img_size, img_size)\n","\n","labels = [\"bg\"] + labels\n","hyper_params[\"total_labels\"] = len(labels)\n","print(train_total_items, val_total_items)\n","print(num_classes, labels)\n","\n","train_data = train_data.map(lambda x : tfr_preprocessing(x, img_size, img_size))\n","val_data = val_data.map(lambda x : tfr_preprocessing(x, img_size, img_size))\n","\n","data_shapes = get_data_shapes()\n","padding_values = get_padding_values()\n","print(data_shapes, padding_values)\n","train_data = train_data.shuffle(batch_size*4).padded_batch(batch_size, padded_shapes=data_shapes, padding_values=padding_values)\n","val_data = val_data.padded_batch(batch_size, padded_shapes=data_shapes, padding_values=padding_values)\n","\n","def transform(dataset):\n","    autotune = tf.data.experimental.AUTOTUNE\n","    dataset = dataset.apply(tf.data.experimental.ignore_errors())\n","    dataset = dataset.repeat(epochs)\n","    dataset = dataset.prefetch(autotune)\n","    return dataset\n","\n","train_data = transform(train_data)\n","\n","ssd_train_feed = generator(train_data, prior_boxes, hyper_params)\n","ssd_val_feed = generator(val_data, prior_boxes, hyper_params)"]},{"cell_type":"markdown","metadata":{"id":"Nj-9WLjVHLhN"},"source":["# trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":671},"execution":{"iopub.execute_input":"2024-02-14T08:04:51.635198Z","iopub.status.busy":"2024-02-14T08:04:51.634335Z"},"executionInfo":{"elapsed":1620867,"status":"error","timestamp":1708762555842,"user":{"displayName":"happy duck","userId":"12590459388487510630"},"user_tz":-480},"id":"tootxTnvHA3t","outputId":"17d58f5a-265d-41ee-8aeb-b770fd5146ec","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"]},{"name":"stdout","output_type":"stream","text":["716 29\n","Epoch 1/200\n","29/29 [==============================] - 7s 164ms/step\n","mAP: 0.0\n","716/716 [==============================] - 596s 739ms/step - loss: 8.3037 - loc_loss: 2.5886 - conf_loss: 5.7151 - val_loss: 6.9568 - val_loc_loss: 2.1443 - val_conf_loss: 4.8125 - lr: 1.0000e-05\n","Epoch 2/200\n","29/29 [==============================] - 7s 162ms/step\n","mAP: 0.0\n","716/716 [==============================] - 422s 589ms/step - loss: 8.2705 - loc_loss: 2.5900 - conf_loss: 5.6805 - val_loss: 6.9096 - val_loc_loss: 2.1436 - val_conf_loss: 4.7660 - lr: 1.0000e-05\n","Epoch 3/200\n","29/29 [==============================] - 8s 165ms/step\n","mAP: 0.0\n","716/716 [==============================] - 417s 583ms/step - loss: 8.0719 - loc_loss: 2.5803 - conf_loss: 5.4915 - val_loss: 6.5556 - val_loc_loss: 2.1434 - val_conf_loss: 4.4123 - lr: 1.0000e-05\n","Epoch 4/200\n","335/716 [=============>................] - ETA: 3:25 - loss: 6.7459 - loc_loss: 2.5463 - conf_loss: 4.1996"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-23d42b7855ef>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                                             batch_size, hyper_params[\"patience\"], ssd_model_path)\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m history = ssd_model.fit(ssd_train_feed,\n\u001b[0m\u001b[1;32m     39\u001b[0m               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mssd_val_feed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n","from tensorflow.keras.optimizers import SGD, Adam\n","\n","ssd_custom_losses = CustomLoss(hyper_params[\"neg_pos_ratio\"], hyper_params[\"loc_loss_alpha\"],\n","                               hyper_params[\"alpha\"], hyper_params[\"gamma\"], hyper_params[\"use_focal\"])\n","\n","if hyper_params[\"detection\"] is None:\n","    ssd_model = get_model(hyper_params)\n","elif hyper_params[\"detection\"] == \"FPN\":\n","    ssd_model = get_fpnmodel(hyper_params)\n","elif hyper_params[\"detection\"] == \"PAFPN\":\n","    ssd_model = get_pafpnmodel(hyper_params)\n","elif hyper_params[\"detection\"] == \"BiFPN\":\n","    ssd_model = get_bifpnmodel(hyper_params)\n","elif hyper_params[\"detection\"] == \"NASFPN\":\n","    ssd_model = get_nasfpnmodel(hyper_params)\n","ssd_model.compile(optimizer=Adam(learning_rate=hyper_params[\"lr\"]),\n","                  loss=[ssd_custom_losses.loc_loss_fn, ssd_custom_losses.conf_loss_fn])\n","init_model(ssd_model, img_size)\n","\n","ssd_model_path = get_model_path(backbone)\n","if load_weights:\n","    ssd_model.load_weights(ssd_model_path)\n","ssd_log_path = get_log_path(backbone)\n","\n","step_size_train = get_step_size(train_total_items, batch_size)\n","step_size_val = get_step_size(val_total_items, batch_size)\n","print(step_size_train, step_size_val)\n","\n","# Moved under mAP_callback\n","# checkpoint_callback = ModelCheckpoint(ssd_model_path, monitor=\"val_loss\", save_best_only=True, save_weights_only=True)\n","learning_rate_callback = LearningRateScheduler(scheduler, verbose=0)\n","tensorboard_callback = TensorBoard(log_dir=ssd_log_path)\n","# mAP_callback both early stopping and checkpoint\n","mAP_callback = MeanAveragePrecisionCallback(val_data, step_size_val, labels, prior_boxes, hyper_params,\n","                                            batch_size, hyper_params[\"patience\"], ssd_model_path)\n","\n","history = ssd_model.fit(ssd_train_feed,\n","              steps_per_epoch=step_size_train,\n","              validation_data=ssd_val_feed,\n","              validation_steps=step_size_val,\n","              epochs=epochs,\n","              callbacks=[tensorboard_callback, learning_rate_callback, mAP_callback],\n","              use_multiprocessing = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2700,"status":"ok","timestamp":1708755561144,"user":{"displayName":"happy duck","userId":"12590459388487510630"},"user_tz":-480},"id":"r8bG8i6PPpkg","outputId":"3b249eab-5dc7-430d-aed3-f9e7d4ace18a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_5 (InputLayer)        [(None, 640, 640, 3)]        0         []                            \n","                                                                                                  \n"," Conv1 (Conv2D)              (None, 320, 320, 32)         864       ['input_5[0][0]']             \n","                                                                                                  \n"," bn_Conv1 (BatchNormalizati  (None, 320, 320, 32)         128       ['Conv1[0][0]']               \n"," on)                                                                                              \n","                                                                                                  \n"," Conv1_relu (ReLU)           (None, 320, 320, 32)         0         ['bn_Conv1[0][0]']            \n","                                                                                                  \n"," expanded_conv_depthwise (D  (None, 320, 320, 32)         288       ['Conv1_relu[0][0]']          \n"," epthwiseConv2D)                                                                                  \n","                                                                                                  \n"," expanded_conv_depthwise_BN  (None, 320, 320, 32)         128       ['expanded_conv_depthwise[0][0\n","  (BatchNormalization)                                              ]']                           \n","                                                                                                  \n"," expanded_conv_depthwise_re  (None, 320, 320, 32)         0         ['expanded_conv_depthwise_BN[0\n"," lu (ReLU)                                                          ][0]']                        \n","                                                                                                  \n"," expanded_conv_project (Con  (None, 320, 320, 16)         512       ['expanded_conv_depthwise_relu\n"," v2D)                                                               [0][0]']                      \n","                                                                                                  \n"," expanded_conv_project_BN (  (None, 320, 320, 16)         64        ['expanded_conv_project[0][0]'\n"," BatchNormalization)                                                ]                             \n","                                                                                                  \n"," block_1_expand (Conv2D)     (None, 320, 320, 96)         1536      ['expanded_conv_project_BN[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," block_1_expand_BN (BatchNo  (None, 320, 320, 96)         384       ['block_1_expand[0][0]']      \n"," rmalization)                                                                                     \n","                                                                                                  \n"," block_1_expand_relu (ReLU)  (None, 320, 320, 96)         0         ['block_1_expand_BN[0][0]']   \n","                                                                                                  \n"," block_1_pad (ZeroPadding2D  (None, 321, 321, 96)         0         ['block_1_expand_relu[0][0]'] \n"," )                                                                                                \n","                                                                                                  \n"," block_1_depthwise (Depthwi  (None, 160, 160, 96)         864       ['block_1_pad[0][0]']         \n"," seConv2D)                                                                                        \n","                                                                                                  \n"," block_1_depthwise_BN (Batc  (None, 160, 160, 96)         384       ['block_1_depthwise[0][0]']   \n"," hNormalization)                                                                                  \n","                                                                                                  \n"," block_1_depthwise_relu (Re  (None, 160, 160, 96)         0         ['block_1_depthwise_BN[0][0]']\n"," LU)                                                                                              \n","                                                                                                  \n"," block_1_project (Conv2D)    (None, 160, 160, 24)         2304      ['block_1_depthwise_relu[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," block_1_project_BN (BatchN  (None, 160, 160, 24)         96        ['block_1_project[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_2_expand (Conv2D)     (None, 160, 160, 144)        3456      ['block_1_project_BN[0][0]']  \n","                                                                                                  \n"," block_2_expand_BN (BatchNo  (None, 160, 160, 144)        576       ['block_2_expand[0][0]']      \n"," rmalization)                                                                                     \n","                                                                                                  \n"," block_2_expand_relu (ReLU)  (None, 160, 160, 144)        0         ['block_2_expand_BN[0][0]']   \n","                                                                                                  \n"," block_2_depthwise (Depthwi  (None, 160, 160, 144)        1296      ['block_2_expand_relu[0][0]'] \n"," seConv2D)                                                                                        \n","                                                                                                  \n"," block_2_depthwise_BN (Batc  (None, 160, 160, 144)        576       ['block_2_depthwise[0][0]']   \n"," hNormalization)                                                                                  \n","                                                                                                  \n"," block_2_depthwise_relu (Re  (None, 160, 160, 144)        0         ['block_2_depthwise_BN[0][0]']\n"," LU)                                                                                              \n","                                                                                                  \n"," block_2_project (Conv2D)    (None, 160, 160, 24)         3456      ['block_2_depthwise_relu[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," block_2_project_BN (BatchN  (None, 160, 160, 24)         96        ['block_2_project[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_2_add (Add)           (None, 160, 160, 24)         0         ['block_1_project_BN[0][0]',  \n","                                                                     'block_2_project_BN[0][0]']  \n","                                                                                                  \n"," block_3_expand (Conv2D)     (None, 160, 160, 144)        3456      ['block_2_add[0][0]']         \n","                                                                                                  \n"," block_3_expand_BN (BatchNo  (None, 160, 160, 144)        576       ['block_3_expand[0][0]']      \n"," rmalization)                                                                                     \n","                                                                                                  \n"," block_3_expand_relu (ReLU)  (None, 160, 160, 144)        0         ['block_3_expand_BN[0][0]']   \n","                                                                                                  \n"," block_3_pad (ZeroPadding2D  (None, 161, 161, 144)        0         ['block_3_expand_relu[0][0]'] \n"," )                                                                                                \n","                                                                                                  \n"," block_3_depthwise (Depthwi  (None, 80, 80, 144)          1296      ['block_3_pad[0][0]']         \n"," seConv2D)                                                                                        \n","                                                                                                  \n"," block_3_depthwise_BN (Batc  (None, 80, 80, 144)          576       ['block_3_depthwise[0][0]']   \n"," hNormalization)                                                                                  \n","                                                                                                  \n"," block_3_depthwise_relu (Re  (None, 80, 80, 144)          0         ['block_3_depthwise_BN[0][0]']\n"," LU)                                                                                              \n","                                                                                                  \n"," block_3_project (Conv2D)    (None, 80, 80, 32)           4608      ['block_3_depthwise_relu[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," block_3_project_BN (BatchN  (None, 80, 80, 32)           128       ['block_3_project[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_4_expand (Conv2D)     (None, 80, 80, 192)          6144      ['block_3_project_BN[0][0]']  \n","                                                                                                  \n"," block_4_expand_BN (BatchNo  (None, 80, 80, 192)          768       ['block_4_expand[0][0]']      \n"," rmalization)                                                                                     \n","                                                                                                  \n"," block_4_expand_relu (ReLU)  (None, 80, 80, 192)          0         ['block_4_expand_BN[0][0]']   \n","                                                                                                  \n"," block_4_depthwise (Depthwi  (None, 80, 80, 192)          1728      ['block_4_expand_relu[0][0]'] \n"," seConv2D)                                                                                        \n","                                                                                                  \n"," block_4_depthwise_BN (Batc  (None, 80, 80, 192)          768       ['block_4_depthwise[0][0]']   \n"," hNormalization)                                                                                  \n","                                                                                                  \n"," block_4_depthwise_relu (Re  (None, 80, 80, 192)          0         ['block_4_depthwise_BN[0][0]']\n"," LU)                                                                                              \n","                                                                                                  \n"," block_4_project (Conv2D)    (None, 80, 80, 32)           6144      ['block_4_depthwise_relu[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," block_4_project_BN (BatchN  (None, 80, 80, 32)           128       ['block_4_project[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_4_add (Add)           (None, 80, 80, 32)           0         ['block_3_project_BN[0][0]',  \n","                                                                     'block_4_project_BN[0][0]']  \n","                                                                                                  \n"," block_5_expand (Conv2D)     (None, 80, 80, 192)          6144      ['block_4_add[0][0]']         \n","                                                                                                  \n"," block_5_expand_BN (BatchNo  (None, 80, 80, 192)          768       ['block_5_expand[0][0]']      \n"," rmalization)                                                                                     \n","                                                                                                  \n"," block_5_expand_relu (ReLU)  (None, 80, 80, 192)          0         ['block_5_expand_BN[0][0]']   \n","                                                                                                  \n"," block_5_depthwise (Depthwi  (None, 80, 80, 192)          1728      ['block_5_expand_relu[0][0]'] \n"," seConv2D)                                                                                        \n","                                                                                                  \n"," block_5_depthwise_BN (Batc  (None, 80, 80, 192)          768       ['block_5_depthwise[0][0]']   \n"," hNormalization)                                                                                  \n","                                                                                                  \n"," block_5_depthwise_relu (Re  (None, 80, 80, 192)          0         ['block_5_depthwise_BN[0][0]']\n"," LU)                                                                                              \n","                                                                                                  \n"," block_5_project (Conv2D)    (None, 80, 80, 32)           6144      ['block_5_depthwise_relu[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," block_5_project_BN (BatchN  (None, 80, 80, 32)           128       ['block_5_project[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_5_add (Add)           (None, 80, 80, 32)           0         ['block_4_add[0][0]',         \n","                                                                     'block_5_project_BN[0][0]']  \n","                                                                                                  \n"," block_6_expand (Conv2D)     (None, 80, 80, 192)          6144      ['block_5_add[0][0]']         \n","                                                                                                  \n"," block_6_expand_BN (BatchNo  (None, 80, 80, 192)          768       ['block_6_expand[0][0]']      \n"," rmalization)                                                                                     \n","                                                                                                  \n"," block_6_expand_relu (ReLU)  (None, 80, 80, 192)          0         ['block_6_expand_BN[0][0]']   \n","                                                                                                  \n"," block_6_pad (ZeroPadding2D  (None, 81, 81, 192)          0         ['block_6_expand_relu[0][0]'] \n"," )                                                                                                \n","                                                                                                  \n"," block_6_depthwise (Depthwi  (None, 40, 40, 192)          1728      ['block_6_pad[0][0]']         \n"," seConv2D)                                                                                        \n","                                                                                                  \n"," block_6_depthwise_BN (Batc  (None, 40, 40, 192)          768       ['block_6_depthwise[0][0]']   \n"," hNormalization)                                                                                  \n","                                                                                                  \n"," block_6_depthwise_relu (Re  (None, 40, 40, 192)          0         ['block_6_depthwise_BN[0][0]']\n"," LU)                                                                                              \n","                                                                                                  \n"," block_6_project (Conv2D)    (None, 40, 40, 64)           12288     ['block_6_depthwise_relu[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," block_6_project_BN (BatchN  (None, 40, 40, 64)           256       ['block_6_project[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_7_expand (Conv2D)     (None, 40, 40, 384)          24576     ['block_6_project_BN[0][0]']  \n","                                                                                                  \n"," block_7_expand_BN (BatchNo  (None, 40, 40, 384)          1536      ['block_7_expand[0][0]']      \n"," rmalization)                                                                                     \n","                                                                                                  \n"," block_7_expand_relu (ReLU)  (None, 40, 40, 384)          0         ['block_7_expand_BN[0][0]']   \n","                                                                                                  \n"," block_7_depthwise (Depthwi  (None, 40, 40, 384)          3456      ['block_7_expand_relu[0][0]'] \n"," seConv2D)                                                                                        \n","                                                                                                  \n"," block_7_depthwise_BN (Batc  (None, 40, 40, 384)          1536      ['block_7_depthwise[0][0]']   \n"," hNormalization)                                                                                  \n","                                                                                                  \n"," block_7_depthwise_relu (Re  (None, 40, 40, 384)          0         ['block_7_depthwise_BN[0][0]']\n"," LU)                                                                                              \n","                                                                                                  \n"," block_7_project (Conv2D)    (None, 40, 40, 64)           24576     ['block_7_depthwise_relu[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," block_7_project_BN (BatchN  (None, 40, 40, 64)           256       ['block_7_project[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_7_add (Add)           (None, 40, 40, 64)           0         ['block_6_project_BN[0][0]',  \n","                                                                     'block_7_project_BN[0][0]']  \n","                                                                                                  \n"," block_8_expand (Conv2D)     (None, 40, 40, 384)          24576     ['block_7_add[0][0]']         \n","                                                                                                  \n"," block_8_expand_BN (BatchNo  (None, 40, 40, 384)          1536      ['block_8_expand[0][0]']      \n"," rmalization)                                                                                     \n","                                                                                                  \n"," block_8_expand_relu (ReLU)  (None, 40, 40, 384)          0         ['block_8_expand_BN[0][0]']   \n","                                                                                                  \n"," block_8_depthwise (Depthwi  (None, 40, 40, 384)          3456      ['block_8_expand_relu[0][0]'] \n"," seConv2D)                                                                                        \n","                                                                                                  \n"," block_8_depthwise_BN (Batc  (None, 40, 40, 384)          1536      ['block_8_depthwise[0][0]']   \n"," hNormalization)                                                                                  \n","                                                                                                  \n"," block_8_depthwise_relu (Re  (None, 40, 40, 384)          0         ['block_8_depthwise_BN[0][0]']\n"," LU)                                                                                              \n","                                                                                                  \n"," block_8_project (Conv2D)    (None, 40, 40, 64)           24576     ['block_8_depthwise_relu[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," block_8_project_BN (BatchN  (None, 40, 40, 64)           256       ['block_8_project[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_8_add (Add)           (None, 40, 40, 64)           0         ['block_7_add[0][0]',         \n","                                                                     'block_8_project_BN[0][0]']  \n","                                                                                                  \n"," block_9_expand (Conv2D)     (None, 40, 40, 384)          24576     ['block_8_add[0][0]']         \n","                                                                                                  \n"," block_9_expand_BN (BatchNo  (None, 40, 40, 384)          1536      ['block_9_expand[0][0]']      \n"," rmalization)                                                                                     \n","                                                                                                  \n"," block_9_expand_relu (ReLU)  (None, 40, 40, 384)          0         ['block_9_expand_BN[0][0]']   \n","                                                                                                  \n"," block_9_depthwise (Depthwi  (None, 40, 40, 384)          3456      ['block_9_expand_relu[0][0]'] \n"," seConv2D)                                                                                        \n","                                                                                                  \n"," block_9_depthwise_BN (Batc  (None, 40, 40, 384)          1536      ['block_9_depthwise[0][0]']   \n"," hNormalization)                                                                                  \n","                                                                                                  \n"," block_9_depthwise_relu (Re  (None, 40, 40, 384)          0         ['block_9_depthwise_BN[0][0]']\n"," LU)                                                                                              \n","                                                                                                  \n"," block_9_project (Conv2D)    (None, 40, 40, 64)           24576     ['block_9_depthwise_relu[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," block_9_project_BN (BatchN  (None, 40, 40, 64)           256       ['block_9_project[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_9_add (Add)           (None, 40, 40, 64)           0         ['block_8_add[0][0]',         \n","                                                                     'block_9_project_BN[0][0]']  \n","                                                                                                  \n"," block_10_expand (Conv2D)    (None, 40, 40, 384)          24576     ['block_9_add[0][0]']         \n","                                                                                                  \n"," block_10_expand_BN (BatchN  (None, 40, 40, 384)          1536      ['block_10_expand[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_10_expand_relu (ReLU  (None, 40, 40, 384)          0         ['block_10_expand_BN[0][0]']  \n"," )                                                                                                \n","                                                                                                  \n"," block_10_depthwise (Depthw  (None, 40, 40, 384)          3456      ['block_10_expand_relu[0][0]']\n"," iseConv2D)                                                                                       \n","                                                                                                  \n"," block_10_depthwise_BN (Bat  (None, 40, 40, 384)          1536      ['block_10_depthwise[0][0]']  \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," block_10_depthwise_relu (R  (None, 40, 40, 384)          0         ['block_10_depthwise_BN[0][0]'\n"," eLU)                                                               ]                             \n","                                                                                                  \n"," block_10_project (Conv2D)   (None, 40, 40, 96)           36864     ['block_10_depthwise_relu[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," block_10_project_BN (Batch  (None, 40, 40, 96)           384       ['block_10_project[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," block_11_expand (Conv2D)    (None, 40, 40, 576)          55296     ['block_10_project_BN[0][0]'] \n","                                                                                                  \n"," block_11_expand_BN (BatchN  (None, 40, 40, 576)          2304      ['block_11_expand[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_11_expand_relu (ReLU  (None, 40, 40, 576)          0         ['block_11_expand_BN[0][0]']  \n"," )                                                                                                \n","                                                                                                  \n"," block_11_depthwise (Depthw  (None, 40, 40, 576)          5184      ['block_11_expand_relu[0][0]']\n"," iseConv2D)                                                                                       \n","                                                                                                  \n"," block_11_depthwise_BN (Bat  (None, 40, 40, 576)          2304      ['block_11_depthwise[0][0]']  \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," block_11_depthwise_relu (R  (None, 40, 40, 576)          0         ['block_11_depthwise_BN[0][0]'\n"," eLU)                                                               ]                             \n","                                                                                                  \n"," block_11_project (Conv2D)   (None, 40, 40, 96)           55296     ['block_11_depthwise_relu[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," block_11_project_BN (Batch  (None, 40, 40, 96)           384       ['block_11_project[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," block_11_add (Add)          (None, 40, 40, 96)           0         ['block_10_project_BN[0][0]', \n","                                                                     'block_11_project_BN[0][0]'] \n","                                                                                                  \n"," block_12_expand (Conv2D)    (None, 40, 40, 576)          55296     ['block_11_add[0][0]']        \n","                                                                                                  \n"," block_12_expand_BN (BatchN  (None, 40, 40, 576)          2304      ['block_12_expand[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_12_expand_relu (ReLU  (None, 40, 40, 576)          0         ['block_12_expand_BN[0][0]']  \n"," )                                                                                                \n","                                                                                                  \n"," block_12_depthwise (Depthw  (None, 40, 40, 576)          5184      ['block_12_expand_relu[0][0]']\n"," iseConv2D)                                                                                       \n","                                                                                                  \n"," block_12_depthwise_BN (Bat  (None, 40, 40, 576)          2304      ['block_12_depthwise[0][0]']  \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," block_12_depthwise_relu (R  (None, 40, 40, 576)          0         ['block_12_depthwise_BN[0][0]'\n"," eLU)                                                               ]                             \n","                                                                                                  \n"," block_12_project (Conv2D)   (None, 40, 40, 96)           55296     ['block_12_depthwise_relu[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," block_12_project_BN (Batch  (None, 40, 40, 96)           384       ['block_12_project[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," block_12_add (Add)          (None, 40, 40, 96)           0         ['block_11_add[0][0]',        \n","                                                                     'block_12_project_BN[0][0]'] \n","                                                                                                  \n"," block_13_expand (Conv2D)    (None, 40, 40, 576)          55296     ['block_12_add[0][0]']        \n","                                                                                                  \n"," block_13_expand_BN (BatchN  (None, 40, 40, 576)          2304      ['block_13_expand[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_13_expand_relu (ReLU  (None, 40, 40, 576)          0         ['block_13_expand_BN[0][0]']  \n"," )                                                                                                \n","                                                                                                  \n"," block_13_pad (ZeroPadding2  (None, 41, 41, 576)          0         ['block_13_expand_relu[0][0]']\n"," D)                                                                                               \n","                                                                                                  \n"," block_13_depthwise (Depthw  (None, 20, 20, 576)          5184      ['block_13_pad[0][0]']        \n"," iseConv2D)                                                                                       \n","                                                                                                  \n"," block_13_depthwise_BN (Bat  (None, 20, 20, 576)          2304      ['block_13_depthwise[0][0]']  \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," block_13_depthwise_relu (R  (None, 20, 20, 576)          0         ['block_13_depthwise_BN[0][0]'\n"," eLU)                                                               ]                             \n","                                                                                                  \n"," block_13_project (Conv2D)   (None, 20, 20, 160)          92160     ['block_13_depthwise_relu[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," block_13_project_BN (Batch  (None, 20, 20, 160)          640       ['block_13_project[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," block_14_expand (Conv2D)    (None, 20, 20, 960)          153600    ['block_13_project_BN[0][0]'] \n","                                                                                                  \n"," block_14_expand_BN (BatchN  (None, 20, 20, 960)          3840      ['block_14_expand[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_14_expand_relu (ReLU  (None, 20, 20, 960)          0         ['block_14_expand_BN[0][0]']  \n"," )                                                                                                \n","                                                                                                  \n"," block_14_depthwise (Depthw  (None, 20, 20, 960)          8640      ['block_14_expand_relu[0][0]']\n"," iseConv2D)                                                                                       \n","                                                                                                  \n"," block_14_depthwise_BN (Bat  (None, 20, 20, 960)          3840      ['block_14_depthwise[0][0]']  \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," block_14_depthwise_relu (R  (None, 20, 20, 960)          0         ['block_14_depthwise_BN[0][0]'\n"," eLU)                                                               ]                             \n","                                                                                                  \n"," block_14_project (Conv2D)   (None, 20, 20, 160)          153600    ['block_14_depthwise_relu[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," block_14_project_BN (Batch  (None, 20, 20, 160)          640       ['block_14_project[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," block_14_add (Add)          (None, 20, 20, 160)          0         ['block_13_project_BN[0][0]', \n","                                                                     'block_14_project_BN[0][0]'] \n","                                                                                                  \n"," block_15_expand (Conv2D)    (None, 20, 20, 960)          153600    ['block_14_add[0][0]']        \n","                                                                                                  \n"," block_15_expand_BN (BatchN  (None, 20, 20, 960)          3840      ['block_15_expand[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_15_expand_relu (ReLU  (None, 20, 20, 960)          0         ['block_15_expand_BN[0][0]']  \n"," )                                                                                                \n","                                                                                                  \n"," block_15_depthwise (Depthw  (None, 20, 20, 960)          8640      ['block_15_expand_relu[0][0]']\n"," iseConv2D)                                                                                       \n","                                                                                                  \n"," block_15_depthwise_BN (Bat  (None, 20, 20, 960)          3840      ['block_15_depthwise[0][0]']  \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," block_15_depthwise_relu (R  (None, 20, 20, 960)          0         ['block_15_depthwise_BN[0][0]'\n"," eLU)                                                               ]                             \n","                                                                                                  \n"," block_15_project (Conv2D)   (None, 20, 20, 160)          153600    ['block_15_depthwise_relu[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," block_15_project_BN (Batch  (None, 20, 20, 160)          640       ['block_15_project[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," block_15_add (Add)          (None, 20, 20, 160)          0         ['block_14_add[0][0]',        \n","                                                                     'block_15_project_BN[0][0]'] \n","                                                                                                  \n"," block_16_expand (Conv2D)    (None, 20, 20, 960)          153600    ['block_15_add[0][0]']        \n","                                                                                                  \n"," block_16_expand_BN (BatchN  (None, 20, 20, 960)          3840      ['block_16_expand[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," block_16_expand_relu (ReLU  (None, 20, 20, 960)          0         ['block_16_expand_BN[0][0]']  \n"," )                                                                                                \n","                                                                                                  \n"," block_16_depthwise (Depthw  (None, 20, 20, 960)          8640      ['block_16_expand_relu[0][0]']\n"," iseConv2D)                                                                                       \n","                                                                                                  \n"," block_16_depthwise_BN (Bat  (None, 20, 20, 960)          3840      ['block_16_depthwise[0][0]']  \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," block_16_depthwise_relu (R  (None, 20, 20, 960)          0         ['block_16_depthwise_BN[0][0]'\n"," eLU)                                                               ]                             \n","                                                                                                  \n"," block_16_project (Conv2D)   (None, 20, 20, 320)          307200    ['block_16_depthwise_relu[0][0\n","                                                                    ]']                           \n","                                                                                                  \n"," block_16_project_BN (Batch  (None, 20, 20, 320)          1280      ['block_16_project[0][0]']    \n"," Normalization)                                                                                   \n","                                                                                                  \n"," Conv_1 (Conv2D)             (None, 20, 20, 1280)         409600    ['block_16_project_BN[0][0]'] \n","                                                                                                  \n"," Conv_1_bn (BatchNormalizat  (None, 20, 20, 1280)         5120      ['Conv_1[0][0]']              \n"," ion)                                                                                             \n","                                                                                                  \n"," out_relu (ReLU)             (None, 20, 20, 1280)         0         ['Conv_1_bn[0][0]']           \n","                                                                                                  \n"," average_pooling2d_28 (Aver  (None, 10, 10, 1280)         0         ['out_relu[0][0]']            \n"," agePooling2D)                                                                                    \n","                                                                                                  \n"," BiFPN (BiFPN)               [(None, 160, 160, 64),       347264    ['block_3_expand_relu[0][0]', \n","                              (None, 80, 80, 64),                    'block_6_expand_relu[0][0]', \n","                              (None, 40, 40, 64),                    'block_13_expand_relu[0][0]',\n","                              (None, 20, 20, 64),                    'out_relu[0][0]',            \n","                              (None, 10, 10, 64)]                    'average_pooling2d_28[0][0]']\n","                                                                                                  \n"," box_regressor (BoxRegresso  multiple                     17112     ['BiFPN[0][0]',               \n"," r)                                                                  'BiFPN[0][1]',               \n","                                                                     'BiFPN[0][2]',               \n","                                                                     'BiFPN[0][3]',               \n","                                                                     'BiFPN[0][4]']               \n","                                                                                                  \n"," class_det (ClassDetector)   multiple                     17502     ['BiFPN[0][0]',               \n","                                                                     'BiFPN[0][1]',               \n","                                                                     'BiFPN[0][2]',               \n","                                                                     'BiFPN[0][3]',               \n","                                                                     'BiFPN[0][4]']               \n","                                                                                                  \n"," tf.reshape_31 (TFOpLambda)  (8, None, 4)                 0         ['box_regressor[0][0]']       \n","                                                                                                  \n"," tf.reshape_33 (TFOpLambda)  (8, None, 4)                 0         ['box_regressor[1][0]']       \n","                                                                                                  \n"," tf.reshape_35 (TFOpLambda)  (8, None, 4)                 0         ['box_regressor[2][0]']       \n","                                                                                                  \n"," tf.reshape_37 (TFOpLambda)  (8, None, 4)                 0         ['box_regressor[3][0]']       \n","                                                                                                  \n"," tf.reshape_39 (TFOpLambda)  (8, None, 4)                 0         ['box_regressor[4][0]']       \n","                                                                                                  \n"," tf.reshape_30 (TFOpLambda)  (8, None, 5)                 0         ['class_det[0][0]']           \n","                                                                                                  \n"," tf.reshape_32 (TFOpLambda)  (8, None, 5)                 0         ['class_det[1][0]']           \n","                                                                                                  \n"," tf.reshape_34 (TFOpLambda)  (8, None, 5)                 0         ['class_det[2][0]']           \n","                                                                                                  \n"," tf.reshape_36 (TFOpLambda)  (8, None, 5)                 0         ['class_det[3][0]']           \n","                                                                                                  \n"," tf.reshape_38 (TFOpLambda)  (8, None, 5)                 0         ['class_det[4][0]']           \n","                                                                                                  \n"," tf.concat_7 (TFOpLambda)    (8, None, 4)                 0         ['tf.reshape_31[0][0]',       \n","                                                                     'tf.reshape_33[0][0]',       \n","                                                                     'tf.reshape_35[0][0]',       \n","                                                                     'tf.reshape_37[0][0]',       \n","                                                                     'tf.reshape_39[0][0]']       \n","                                                                                                  \n"," tf.concat_6 (TFOpLambda)    (8, None, 5)                 0         ['tf.reshape_30[0][0]',       \n","                                                                     'tf.reshape_32[0][0]',       \n","                                                                     'tf.reshape_34[0][0]',       \n","                                                                     'tf.reshape_36[0][0]',       \n","                                                                     'tf.reshape_38[0][0]']       \n","                                                                                                  \n"," tf.concat_8 (TFOpLambda)    (8, None, 9)                 0         ['tf.concat_7[0][0]',         \n","                                                                     'tf.concat_6[0][0]']         \n","                                                                                                  \n","==================================================================================================\n","Total params: 2639862 (10.07 MB)\n","Trainable params: 2601270 (9.92 MB)\n","Non-trainable params: 38592 (150.75 KB)\n","__________________________________________________________________________________________________\n"]}],"source":["ssd_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"nVPhU6-xkSF5"},"source":["# Tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4HPimys1Wfv"},"outputs":[],"source":["plt.figure(1)\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'valid'], loc='upper left')\n","\n","plt.figure(2)\n","plt.plot(history.history['loc_loss'])\n","plt.plot(history.history['val_loc_loss'])\n","plt.title('model loss')\n","plt.ylabel('loc loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'valid'], loc='upper left')\n","\n","plt.figure(3)\n","plt.plot(history.history['conf_loss'])\n","plt.plot(history.history['val_conf_loss'])\n","plt.title('model loss')\n","plt.ylabel('conf loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'valid'], loc='upper left')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-YoXADJikWs6"},"outputs":[],"source":["mAP_values = mAP_callback.get_mAP_values()\n","plt.plot(mAP_values)\n","plt.title('mAP')\n","plt.xlabel('epoch')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KDTcajkTw4DG"},"outputs":[],"source":["import glob\n","\n","logs = os.listdir(f\"/content/logs/{backbone}/\")\n","logs.sort()\n","log_path = f\"/content/logs/{backbone}/{logs[-1]}\"\n","print(log_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hk0iUI8PreZR"},"outputs":[],"source":["# # %load_ext tensorboard\n","# %reload_ext tensorboard\n","# %tensorboard --logdir \"/content/logs/mobilenet_v2/20240129-090642\""]},{"cell_type":"markdown","metadata":{"id":"yNKy8TbeM4AA"},"source":["# predictor (test mAP + FPS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9sYYPOeNoy0"},"outputs":[],"source":["evaluate = True\n","use_custom_images = False\n","custom_image_path = \"data/images/\"\n","\n","test_data, total_items = tfr_dataset(test_files)\n","print(total_items)\n","\n","data_types = get_data_types()\n","data_shapes = get_data_shapes()\n","padding_values = get_padding_values()\n","\n","if use_custom_images:\n","    img_paths = get_custom_imgs(custom_image_path)\n","    total_items = len(img_paths)\n","    test_data = tf.data.Dataset.from_generator(lambda: custom_data_generator(\n","                                               img_paths, img_size, img_size), data_types, data_shapes)\n","else:\n","    test_data = test_data.map(lambda x : tfr_preprocessing(x, img_size, img_size))\n","\n","test_data = test_data.padded_batch(batch_size, padded_shapes=data_shapes, padding_values=padding_values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGcBVI4Gn_Mz"},"outputs":[],"source":["import time\n","\n","# if hyper_params[\"detection\"] is None:\n","#     ssd_model = get_model(hyper_params)\n","# elif hyper_params[\"detection\"] == \"FPN\":\n","#     ssd_model = get_fpnmodel(hyper_params)\n","# elif hyper_params[\"detection\"] == \"PAFPN\":\n","#     ssd_model = get_pafpnmodel(hyper_params)\n","# elif hyper_params[\"detection\"] == \"BiFPN\":\n","#     ssd_model = get_bifpnmodel(hyper_params)\n","# elif hyper_params[\"detection\"] == \"NASFPN\":\n","#     ssd_model = get_nasfpnmodel(hyper_params)\n","# ssd_model_path = get_model_path(backbone)\n","# ssd_model.load_weights(ssd_model_path)\n","ssd_decoder_model = get_decoder_model(ssd_model, prior_boxes, hyper_params)\n","\n","step_size = get_step_size(total_items, batch_size)\n","\n","start_time = time.time()\n","pred_bboxes, pred_labels, pred_scores = ssd_decoder_model.predict(test_data, steps=step_size, verbose=1)\n","end_time = time.time()\n","total_time_taken = end_time - start_time\n","\n","# Calculate Frames Per Second (FPS)\n","fps = total_items / total_time_taken\n","\n","# Print the results\n","print(f\"Total time taken: {total_time_taken} seconds\")\n","print(f\"Frames Per Second (FPS): {fps}\")\n","\n","if evaluate:\n","    stats, mAP = evaluate_predictions(test_data, pred_bboxes, pred_labels, pred_scores, labels, batch_size)\n","else:\n","    draw_predictions(test_data, pred_bboxes, pred_labels, pred_scores, labels, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nn03Kvkm3UlW"},"outputs":[],"source":["for i in stats:\n","    print(stats[i]['label'])\n","    print('Total:', stats[i]['total'])\n","    print('Total Predictions', len(stats[i]['tp']))\n","    print('Correct Predictions:', stats[i]['tp'].count(1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eAcOo-yBBG9L"},"outputs":[],"source":["stats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itzp-sJcSswM"},"outputs":[],"source":["# draw_predictions(test_data, pred_bboxes, pred_labels, pred_scores, labels, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWtZLX68Zx4-"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1EjObPAbR41QnPNy_eFhmeWQVSiQSgiJ3","timestamp":1708006496588}],"toc_visible":true},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
